//! GPU-Accelerated Path Integral Monte Carlo
//!
//! # Purpose
//! GPU implementation of PIMC for massive speedup in quantum annealing
//!
//! # Constitution Reference
//! Phase 6 Implementation Constitution - Sprint 1.3

use cudarc::driver::*;
use std::sync::Arc;
use anyhow::{Result, Context};

use crate::cma::{Solution, CausalManifold};
use super::path_integral::ProblemHamiltonian;

/// GPU-accelerated PIMC
pub struct GpuPathIntegralMonteCarlo {
    device: Arc<CudaContext>,
    module: Arc<CudaModule>,
    n_beads: usize,
    beta: f64,
}

impl GpuPathIntegralMonteCarlo {
    /// Create new GPU PIMC annealer
    pub fn new(n_beads: usize, beta: f64) -> Result<Self> {
        let device = CudaContext::new(0)
            .context("Failed to initialize CUDA for PIMC")?;

        // Load PIMC kernels
        let ptx_path = std::path::Path::new("target/ptx/pimc_kernels.ptx");
        let ptx = if ptx_path.exists() {
            std::fs::read_to_string(ptx_path)?
        } else {
            // Compile at runtime if needed
            Self::compile_pimc_kernels()?
        };

        let module = device.load_module(ptx.into())?;

        println!("✓ GPU PIMC initialized ({} beads, β={:.2})", n_beads, beta);

        Ok(Self {
            device,
            module,
            n_beads,
            beta,
        })
    }

    /// Quantum anneal on GPU
    pub fn quantum_anneal_gpu(
        &self,
        hamiltonian: &ProblemHamiltonian,
        manifold: &CausalManifold,
        initial: &Solution,
    ) -> Result<Solution> {
        let n_dim = initial.data.len();

        // Allocate GPU memory for path
        let path_size = self.n_beads * n_dim;
        let mut path_data = vec![0.0f32; path_size];

        // Initialize path with replicated initial solution
        for bead in 0..self.n_beads {
            for dim in 0..n_dim {
                path_data[bead * n_dim + dim] = initial.data[dim] as f32;
            }
        }

        let stream = self.device.default_stream();
        let path_gpu = stream.memcpy_stod(&path_data)?;

        // Allocate Hamiltonian matrix on GPU
        let h_matrix = self.hamiltonian_to_matrix(hamiltonian, n_dim);
        let h_gpu = stream.memcpy_stod(&h_matrix)?;

        // Allocate manifold edges on GPU
        let manifold_data = self.manifold_to_gpu_format(manifold);
        let manifold_gpu = stream.memcpy_stod(&manifold_data)?;

        // Initialize random states
        let rand_states_gpu = stream.alloc_zeros::<u8>(path_size * 48)?; // curandState is 48 bytes
        let init_rand = self.module.load_function("init_rand_states_kernel")?;

        let init_config = LaunchConfig {
            grid_dim: (((path_size + 255) / 256) as u32, 1, 1),
            block_dim: (256, 1, 1),
            shared_mem_bytes: 0,
        };

        let mut launch_args = stream.launch_builder(&init_rand);
        launch_args.arg(&rand_states_gpu);
        launch_args.arg(&42u64);
        let path_size_i32 = path_size as i32;
        launch_args.arg(&path_size_i32);

        unsafe {
            launch_args.launch(init_config)?;
        }

        // Annealing loop
        let n_steps = 1000;
        let mut accepted_moves_gpu = stream.alloc_zeros::<i32>(1)?;

        let update_func = self.module.load_function("update_beads_kernel")?;

        for step in 0..n_steps {
            let t = step as f64 / n_steps as f64;
            let beta_t = (self.beta * t) as f32;
            let tau = (self.beta / self.n_beads as f64) as f32;
            let tunneling = ((1.0 - t).powi(2)) as f32;

            let config = LaunchConfig {
                grid_dim: (self.n_beads as u32, 1, 1),
                block_dim: (n_dim.min(256) as u32, 1, 1),
                shared_mem_bytes: 0,
            };

            let mut launch_args = stream.launch_builder(&update_func);
            launch_args.arg(&path_gpu);
            launch_args.arg(&h_gpu);
            launch_args.arg(&manifold_gpu);
            let manifold_edges = manifold.edges.len() as i32;
            launch_args.arg(&manifold_edges);
            let n_beads_i32 = self.n_beads as i32;
            launch_args.arg(&n_beads_i32);
            let n_dim_i32 = n_dim as i32;
            launch_args.arg(&n_dim_i32);
            launch_args.arg(&beta_t);
            launch_args.arg(&tau);
            let mass = 1.0f32;
            launch_args.arg(&mass);
            launch_args.arg(&tunneling);
            launch_args.arg(&rand_states_gpu);
            launch_args.arg(&accepted_moves_gpu);

            unsafe {
                launch_args.launch(config)?;
            }

            // Periodically check progress
            if step % 100 == 0 {
                let accepted: Vec<i32> = stream.memcpy_dtov(&accepted_moves_gpu)?;

                if step > 0 {
                    let acceptance_rate = accepted[0] as f64 / (100.0 * path_size as f64);
                    println!("  Step {}: β={:.3}, tunneling={:.3}, acceptance={:.1}%",
                             step, beta_t, tunneling, acceptance_rate * 100.0);
                }

                // Reset counter by creating new buffer
                accepted_moves_gpu = stream.alloc_zeros::<i32>(1)?;
            }
        }

        // Copy result back
        let final_path: Vec<f32> = stream.memcpy_dtov(&path_gpu)?;

        // Extract best configuration
        self.extract_best_solution(&final_path, hamiltonian, n_dim)
    }

    fn hamiltonian_to_matrix(&self, hamiltonian: &ProblemHamiltonian, n_dim: usize) -> Vec<f32> {
        // Convert Hamiltonian to matrix form for GPU
        // For now, use identity (would probe actual function in production)
        let mut matrix = vec![0.0f32; n_dim * n_dim];

        for i in 0..n_dim {
            matrix[i * n_dim + i] = 1.0;
        }

        matrix
    }

    fn manifold_to_gpu_format(&self, manifold: &CausalManifold) -> Vec<f32> {
        // Convert edges to flat array: [src, tgt, TE, coupling] per edge
        let mut data = Vec::with_capacity(manifold.edges.len() * 4);

        for edge in &manifold.edges {
            data.push(edge.source as f32);
            data.push(edge.target as f32);
            data.push(edge.transfer_entropy as f32);
            data.push(0.1f32); // coupling strength
        }

        data
    }

    fn extract_best_solution(
        &self,
        path: &[f32],
        hamiltonian: &ProblemHamiltonian,
        n_dim: usize,
    ) -> Result<Solution> {
        // Find lowest energy bead
        let mut best_cost = f64::MAX;
        let mut best_data = vec![0.0; n_dim];

        for bead in 0..self.n_beads {
            let start = bead * n_dim;
            let bead_data: Vec<f64> = path[start..start + n_dim]
                .iter()
                .map(|&x| x as f64)
                .collect();

            let solution = Solution {
                data: bead_data.clone(),
                cost: 0.0,
            };

            let cost = hamiltonian.evaluate(&solution);

            if cost < best_cost {
                best_cost = cost;
                best_data = bead_data;
            }
        }

        Ok(Solution {
            data: best_data,
            cost: best_cost,
        })
    }

    fn compile_pimc_kernels() -> Result<String> {
        let cuda_source = include_str!("../cuda/pimc_kernels.cu");
        std::fs::write("/tmp/pimc_kernels.cu", cuda_source)?;

        let output = std::process::Command::new("nvcc")
            .args(&[
                "--ptx",
                "-O3",
                "-arch=sm_86",
                "/tmp/pimc_kernels.cu",
                "-o", "/tmp/pimc_kernels.ptx"
            ])
            .output()?;

        if !output.status.success() {
            anyhow::bail!("PIMC kernel compilation failed");
        }

        std::fs::read_to_string("/tmp/pimc_kernels.ptx")
            .context("Failed to read compiled PIMC PTX")
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_gpu_pimc_creation() {
        let result = GpuPathIntegralMonteCarlo::new(20, 10.0);

        match result {
            Ok(pimc) => {
                println!("✓ GPU PIMC created");
                assert_eq!(pimc.n_beads, 20);
                assert_eq!(pimc.beta, 10.0);
            },
            Err(e) => {
                println!("⚠️  No GPU: {}", e);
            }
        }
    }

    #[test]
    fn test_gpu_pimc_optimization() {
        let result = GpuPathIntegralMonteCarlo::new(10, 5.0);

        if result.is_err() {
            println!("⚠️  Skipping GPU test - no CUDA");
            return;
        }

        let gpu_pimc = result.unwrap();

        let hamiltonian = ProblemHamiltonian::new(
            |s: &Solution| s.data.iter().map(|x| x.powi(2)).sum(),
            0.1,
        );

        let initial = Solution {
            data: vec![2.0, 2.0, 2.0],
            cost: 12.0,
        };

        let manifold = CausalManifold {
            edges: Vec::new(),
            intrinsic_dim: 3,
            metric_tensor: ndarray::Array2::eye(3),
        };

        let result = gpu_pimc.quantum_anneal_gpu(&hamiltonian, &manifold, &initial);

        match result {
            Ok(solution) => {
                println!("✓ GPU PIMC optimization:");
                println!("  Initial: {:.4}", initial.cost);
                println!("  Final: {:.4}", solution.cost);

                assert!(solution.cost <= initial.cost);
            },
            Err(e) => {
                println!("GPU PIMC failed: {}", e);
            }
        }
    }
}