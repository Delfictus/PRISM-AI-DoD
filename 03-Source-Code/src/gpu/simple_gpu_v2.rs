//! GPU Implementation that Actually Uses the GPU
//!
//! This version properly initializes CUDA and performs operations on GPU

use anyhow::{Result, Context};
use std::sync::Arc;

#[cfg(feature = "cuda")]
use cudarc::{
    driver::{CudaContext, CudaStream, CudaSlice, DeviceSlice},
    nvrtc::{Ptx, CompileOptions},
};

/// GPU buffer that actually lives on GPU
pub struct GpuBuffer {
    #[cfg(feature = "cuda")]
    device_ptr: Option<CudaSlice<f32>>,

    #[cfg(not(feature = "cuda"))]
    cpu_data: Vec<f32>,

    size: usize,
}

impl GpuBuffer {
    /// Create new GPU buffer
    pub fn new(ctx: &GpuContext, size: usize) -> Result<Self> {
        #[cfg(feature = "cuda")]
        {
            if let Some(cuda_ctx) = &ctx.cuda_context {
                let device_ptr = cuda_ctx.alloc_zeros::<f32>(size)
                    .context("Failed to allocate GPU memory")?;
                return Ok(Self {
                    device_ptr: Some(device_ptr),
                    size,
                });
            }
        }

        // CPU fallback
        #[cfg(not(feature = "cuda"))]
        {
            Ok(Self {
                cpu_data: vec![0.0f32; size],
                size,
            })
        }

        #[cfg(feature = "cuda")]
        Ok(Self {
            device_ptr: None,
            size,
        })
    }

    /// Upload data from CPU to GPU
    pub fn upload(&mut self, ctx: &GpuContext, data: &[f32]) -> Result<()> {
        #[cfg(feature = "cuda")]
        {
            if let Some(cuda_ctx) = &ctx.cuda_context {
                if let Some(ref mut device_ptr) = self.device_ptr {
                    cuda_ctx.htod_copy_into(data, device_ptr)
                        .context("Failed to copy data to GPU")?;
                    return Ok(());
                }
            }
        }

        // CPU fallback
        #[cfg(not(feature = "cuda"))]
        {
            self.cpu_data = data.to_vec();
        }

        Ok(())
    }

    /// Download data from GPU to CPU
    pub fn download(&self, ctx: &GpuContext) -> Result<Vec<f32>> {
        #[cfg(feature = "cuda")]
        {
            if let Some(cuda_ctx) = &ctx.cuda_context {
                if let Some(ref device_ptr) = self.device_ptr {
                    let mut host_data = vec![0.0f32; self.size];
                    cuda_ctx.dtoh_copy_into(device_ptr, &mut host_data)
                        .context("Failed to copy data from GPU")?;
                    return Ok(host_data);
                }
            }
        }

        // CPU fallback
        #[cfg(not(feature = "cuda"))]
        {
            return Ok(self.cpu_data.clone());
        }

        #[cfg(feature = "cuda")]
        Ok(vec![0.0f32; self.size])
    }
}

/// GPU context that actually initializes CUDA
pub struct GpuContext {
    #[cfg(feature = "cuda")]
    cuda_context: Option<Arc<CudaContext>>,

    #[cfg(feature = "cuda")]
    stream: Option<CudaStream>,

    gpu_available: bool,
}

impl GpuContext {
    /// Create new GPU context - ACTUALLY TRIES TO USE GPU
    pub fn new() -> Result<Self> {
        #[cfg(feature = "cuda")]
        {
            // Actually try to create CUDA context
            match CudaContext::new(0) {
                Ok(ctx) => {
                    println!("‚úÖ GPU ENABLED: Successfully created CUDA context");
                    let stream = ctx.stream()
                        .context("Failed to create CUDA stream")?;

                    return Ok(Self {
                        cuda_context: Some(ctx),
                        stream: Some(stream),
                        gpu_available: true,  // ‚Üê GPU IS NOW ENABLED!
                    });
                }
                Err(e) => {
                    eprintln!("‚ö†Ô∏è GPU initialization failed, using CPU: {}", e);
                }
            }
        }

        #[cfg(not(feature = "cuda"))]
        {
            println!("‚ö†Ô∏è CUDA feature not enabled, using CPU");
        }

        // CPU fallback
        Ok(Self {
            #[cfg(feature = "cuda")]
            cuda_context: None,
            #[cfg(feature = "cuda")]
            stream: None,
            gpu_available: false,
        })
    }

    /// Check if GPU is available
    pub fn is_gpu_available(&self) -> bool {
        self.gpu_available
    }

    /// Get CUDA context
    #[cfg(feature = "cuda")]
    pub fn cuda_context(&self) -> Option<&Arc<CudaContext>> {
        self.cuda_context.as_ref()
    }
}

/// GPU Tensor that actually uses GPU
pub struct GpuTensor {
    buffer: GpuBuffer,
    shape: Vec<usize>,
    context: Arc<GpuContext>,
}

impl GpuTensor {
    /// Create from CPU data - ACTUALLY UPLOADS TO GPU
    pub fn from_cpu(data: Vec<f32>, shape: Vec<usize>) -> Result<Self> {
        let context = Arc::new(GpuContext::new()?);
        let size = data.len();

        let mut buffer = GpuBuffer::new(&context, size)?;
        buffer.upload(&context, &data)?;

        if context.is_gpu_available() {
            println!("  üìä Tensor created on GPU (size: {})", size);
        }

        Ok(Self {
            buffer,
            shape,
            context,
        })
    }

    /// Create zeros tensor
    pub fn zeros(shape: Vec<usize>) -> Result<Self> {
        let size: usize = shape.iter().product();
        let data = vec![0.0f32; size];
        Self::from_cpu(data, shape)
    }

    /// Download to CPU
    pub fn to_cpu(&self) -> Result<Vec<f32>> {
        self.buffer.download(&self.context)
    }

    /// Matrix multiply - USES GPU WHEN AVAILABLE
    pub fn matmul(&self, other: &GpuTensor) -> Result<GpuTensor> {
        if self.shape.len() != 2 || other.shape.len() != 2 {
            anyhow::bail!("matmul requires 2D tensors");
        }

        let m = self.shape[0];
        let k = self.shape[1];
        let n = other.shape[1];

        if k != other.shape[0] {
            anyhow::bail!("Shape mismatch for matmul");
        }

        #[cfg(feature = "cuda")]
        {
            if self.context.is_gpu_available() {
                // Try to use GPU kernel
                if let Some(cuda_ctx) = self.context.cuda_context() {
                    // For now, compile and run a simple kernel
                    let result = self.matmul_gpu(other, cuda_ctx, m, k, n);
                    if let Ok(tensor) = result {
                        println!("  üöÄ Matrix multiply executed on GPU!");
                        return Ok(tensor);
                    }
                }
            }
        }

        // CPU fallback
        println!("  üêå Matrix multiply on CPU (GPU not available)");
        self.matmul_cpu(other, m, k, n)
    }

    /// GPU matrix multiplication
    #[cfg(feature = "cuda")]
    fn matmul_gpu(
        &self,
        other: &GpuTensor,
        ctx: &Arc<CudaContext>,
        m: usize,
        k: usize,
        n: usize,
    ) -> Result<GpuTensor> {
        // Simple GPU kernel for matrix multiplication
        let kernel_code = r#"
extern "C" __global__ void matmul(
    const float* A,
    const float* B,
    float* C,
    int M,
    int K,
    int N
) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < N) {
        float sum = 0.0f;
        for (int i = 0; i < K; i++) {
            sum += A[row * K + i] * B[i * N + col];
        }
        C[row * N + col] = sum;
    }
}
"#;

        // Compile kernel
        let ptx = Ptx::compile_ptx(kernel_code, CompileOptions::default())
            .context("Failed to compile CUDA kernel")?;

        ctx.load_ptx(ptx, "matmul", &["matmul"])
            .context("Failed to load PTX")?;

        // Allocate output
        let output_size = m * n;
        let mut output_buffer = GpuBuffer::new(&self.context, output_size)?;

        // Get device pointers
        let a_ptr = self.buffer.device_ptr.as_ref()
            .ok_or_else(|| anyhow::anyhow!("No device pointer for A"))?;
        let b_ptr = other.buffer.device_ptr.as_ref()
            .ok_or_else(|| anyhow::anyhow!("No device pointer for B"))?;
        let c_ptr = output_buffer.device_ptr.as_mut()
            .ok_or_else(|| anyhow::anyhow!("No device pointer for C"))?;

        // Launch kernel
        let block_size = 16;
        let grid_x = (n + block_size - 1) / block_size;
        let grid_y = (m + block_size - 1) / block_size;

        let kernel = ctx.get_func("matmul", "matmul")
            .context("Failed to get kernel function")?;

        // Launch with proper parameters
        unsafe {
            kernel.launch_async_unchecked(
                &self.context.stream.as_ref().unwrap(),
                (grid_x, grid_y, 1),
                (block_size, block_size, 1),
                &mut [
                    a_ptr as &DeviceSlice<f32> as *const _ as *mut std::ffi::c_void,
                    b_ptr as &DeviceSlice<f32> as *const _ as *mut std::ffi::c_void,
                    c_ptr as &mut DeviceSlice<f32> as *mut _ as *mut std::ffi::c_void,
                    &(m as i32) as *const i32 as *mut std::ffi::c_void,
                    &(k as i32) as *const i32 as *mut std::ffi::c_void,
                    &(n as i32) as *const i32 as *mut std::ffi::c_void,
                ],
            )?;
        }

        // Wait for completion
        self.context.stream.as_ref().unwrap().synchronize()?;

        Ok(GpuTensor {
            buffer: output_buffer,
            shape: vec![m, n],
            context: self.context.clone(),
        })
    }

    /// CPU matrix multiplication fallback
    fn matmul_cpu(&self, other: &GpuTensor, m: usize, k: usize, n: usize) -> Result<GpuTensor> {
        let a_data = self.to_cpu()?;
        let b_data = other.to_cpu()?;
        let mut c_data = vec![0.0f32; m * n];

        for i in 0..m {
            for j in 0..n {
                let mut sum = 0.0f32;
                for l in 0..k {
                    sum += a_data[i * k + l] * b_data[l * n + j];
                }
                c_data[i * n + j] = sum;
            }
        }

        GpuTensor::from_cpu(c_data, vec![m, n])
    }

    /// ReLU activation - USES GPU WHEN AVAILABLE
    pub fn relu(&mut self) -> Result<()> {
        #[cfg(feature = "cuda")]
        {
            if self.context.is_gpu_available() {
                if let Some(cuda_ctx) = self.context.cuda_context() {
                    if self.relu_gpu(cuda_ctx).is_ok() {
                        println!("  üöÄ ReLU executed on GPU!");
                        return Ok(());
                    }
                }
            }
        }

        // CPU fallback
        println!("  üêå ReLU on CPU");
        self.relu_cpu()
    }

    #[cfg(feature = "cuda")]
    fn relu_gpu(&mut self, ctx: &Arc<CudaContext>) -> Result<()> {
        let kernel_code = r#"
extern "C" __global__ void relu(float* data, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        data[idx] = fmaxf(0.0f, data[idx]);
    }
}
"#;

        let ptx = Ptx::compile_ptx(kernel_code, CompileOptions::default())?;
        ctx.load_ptx(ptx, "relu", &["relu"])?;

        let size = self.buffer.size;
        let data_ptr = self.buffer.device_ptr.as_mut()
            .ok_or_else(|| anyhow::anyhow!("No device pointer"))?;

        let kernel = ctx.get_func("relu", "relu")?;
        let block_size = 256;
        let grid_size = (size + block_size - 1) / block_size;

        unsafe {
            kernel.launch_async_unchecked(
                &self.context.stream.as_ref().unwrap(),
                (grid_size, 1, 1),
                (block_size, 1, 1),
                &mut [
                    data_ptr as &mut DeviceSlice<f32> as *mut _ as *mut std::ffi::c_void,
                    &(size as i32) as *const i32 as *mut std::ffi::c_void,
                ],
            )?;
        }

        self.context.stream.as_ref().unwrap().synchronize()?;
        Ok(())
    }

    fn relu_cpu(&mut self) -> Result<()> {
        let mut data = self.to_cpu()?;
        for x in &mut data {
            *x = x.max(0.0);
        }
        self.buffer.upload(&self.context, &data)?;
        Ok(())
    }

    /// Get shape
    pub fn shape(&self) -> &[usize] {
        &self.shape
    }
}

/// Linear layer that uses GPU
pub struct GpuLinear {
    weight: GpuTensor,
    bias: GpuTensor,
    in_features: usize,
    out_features: usize,
}

impl GpuLinear {
    pub fn new(in_features: usize, out_features: usize) -> Result<Self> {
        use rand::Rng;
        let mut rng = rand::thread_rng();

        let scale = (2.0 / in_features as f32).sqrt();
        let weight_data: Vec<f32> = (0..in_features * out_features)
            .map(|_| rng.gen_range(-scale..scale))
            .collect();

        let weight = GpuTensor::from_cpu(weight_data, vec![in_features, out_features])?;
        let bias = GpuTensor::zeros(vec![out_features])?;

        Ok(Self {
            weight,
            bias,
            in_features,
            out_features,
        })
    }

    pub fn forward(&self, input: &GpuTensor) -> Result<GpuTensor> {
        // Matrix multiply
        let mut output = input.matmul(&self.weight)?;

        // Add bias (simplified - just download/upload for now)
        let mut output_data = output.to_cpu()?;
        let bias_data = self.bias.to_cpu()?;

        let batch_size = output.shape[0];
        let features = output.shape[1];

        for b in 0..batch_size {
            for f in 0..features {
                output_data[b * features + f] += bias_data[f];
            }
        }

        GpuTensor::from_cpu(output_data, output.shape.clone())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_gpu_context_creation() {
        let ctx = GpuContext::new().unwrap();
        println!("GPU available: {}", ctx.is_gpu_available());
    }

    #[test]
    fn test_gpu_tensor() {
        let data = vec![1.0, 2.0, 3.0, 4.0];
        let tensor = GpuTensor::from_cpu(data.clone(), vec![2, 2]).unwrap();
        let result = tensor.to_cpu().unwrap();
        assert_eq!(result, data);
    }

    #[test]
    fn test_gpu_matmul() {
        let a = GpuTensor::from_cpu(vec![1.0, 2.0, 3.0, 4.0], vec![2, 2]).unwrap();
        let b = GpuTensor::from_cpu(vec![5.0, 6.0, 7.0, 8.0], vec![2, 2]).unwrap();

        let c = a.matmul(&b).unwrap();
        let result = c.to_cpu().unwrap();

        // [1,2] * [5,6]   = [1*5+2*7, 1*6+2*8] = [19, 22]
        // [3,4]   [7,8]     [3*5+4*7, 3*6+4*8]   [43, 50]
        assert_eq!(result, vec![19.0, 22.0, 43.0, 50.0]);
    }
}