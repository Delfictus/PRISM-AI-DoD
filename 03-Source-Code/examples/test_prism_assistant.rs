//! Test PRISM Assistant - Local LLM Integration
//!
//! This example demonstrates the PRISM Assistant with local GPU LLM.
//!
//! Usage:
//! ```bash
//! cargo run --example test_prism_assistant --features cuda
//! ```

use prism_ai::assistant::{PrismAssistant, AssistantMode};
use anyhow::Result;

#[tokio::main]
async fn main() -> Result<()> {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘  PRISM ASSISTANT TEST                    â•‘");
    println!("â•‘  Local GPU LLM Integration               â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // Test 1: Create assistant with local model
    println!("ğŸ”§ Test 1: Creating PRISM Assistant in LocalOnly mode...\n");

    let model_path = Some("/home/diddy/Desktop/PRISM-Worker-6/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf".to_string());

    let mut assistant = PrismAssistant::new(
        AssistantMode::LocalOnly,
        model_path
    ).await?;

    println!("âœ… Assistant created successfully!\n");

    // Test 2: Simple chat
    println!("ğŸ”§ Test 2: Testing simple chat...\n");

    let response = assistant.chat("Hello! What is 2 + 2?").await?;

    println!("ğŸ“¥ User: Hello! What is 2 + 2?");
    println!("ğŸ“¤ Assistant: {}", response.text);
    println!("â±ï¸  Latency: {}ms", response.latency_ms);
    println!("ğŸ’° Cost: ${:.4}", response.cost_usd);
    println!("ğŸ¤– Model: {}", response.model);
    println!();

    // Test 3: Status check
    println!("ğŸ”§ Test 3: Checking assistant status...\n");

    let status = assistant.status();
    println!("Status: {:?}\n", status);

    println!("âœ… All tests completed successfully!");

    Ok(())
}
