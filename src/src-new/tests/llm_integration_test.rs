//! Integration tests for Local LLM pipeline
//!
//! Tests the complete workflow: GGUF loading -> Tokenization -> KV-cache -> Sampling

use prism_ai::orchestration::local_llm::{
    GgufLoader, GgufType, BPETokenizer, TokenSampler, SamplingConfig,
    LayerKVCache, TransformerKVCache,
};
use anyhow::Result;
use std::collections::HashMap;

#[test]
fn test_full_llm_pipeline_simulation() -> Result<()> {
    // This test simulates a complete LLM inference pipeline
    // without requiring an actual model file or GPU

    // 1. Simulate GGUF metadata (what we'd get from a real model)
    let vocab_size = 32000;
    let n_layers = 32;
    let n_heads = 32;
    let d_model = 4096;
    let context_length = 2048;

    // 2. Create tokenizer
    let tokenizer = BPETokenizer::new(vocab_size);

    // Test encoding
    let text = "Hello, world!";
    let token_ids = tokenizer.encode(text, true)?;
    assert!(!token_ids.is_empty(), "Tokenization should produce tokens");

    // Test decoding
    let decoded = tokenizer.decode(&token_ids)?;
    assert!(!decoded.is_empty(), "Decoding should produce text");

    // 3. Simulate KV-cache (without GPU)
    let batch_size = 1;
    let max_seq_len = 512;

    // Verify cache can be created with model parameters
    assert!(n_layers > 0);
    assert!(d_model > 0);
    assert!(max_seq_len > 0);
    assert!(batch_size > 0);

    // 4. Test sampling strategies
    let sampler = TokenSampler::new(SamplingConfig::standard());

    // Simulate logits from model (vocab_size probabilities)
    let mut logits = vec![0.0; vocab_size];
    logits[100] = 2.0;  // High logit for token 100
    logits[200] = 1.5;  // Medium logit for token 200
    logits[300] = 1.0;  // Lower logit for token 300

    // Sample next token
    let next_token = sampler.sample(&logits, &[])?;
    assert!(next_token >= 0 && next_token < vocab_size as i32,
            "Sampled token should be in valid range");

    // 5. Test complete generation loop simulation
    let mut generated_tokens = vec![];
    let mut previous_tokens = vec![];
    let max_tokens = 10;

    for _ in 0..max_tokens {
        // In a real implementation:
        // - Load model weights with GGUF loader
        // - Run forward pass through transformer
        // - Use KV-cache for efficiency
        // - Sample next token

        // Simulate varying logits
        for logit in logits.iter_mut() {
            *logit += (rand::random::<f32>() - 0.5) * 0.1;
        }

        let token = sampler.sample(&logits, &previous_tokens)?;
        generated_tokens.push(token);
        previous_tokens.push(token);

        // Test repetition penalty is working
        if previous_tokens.len() > 3 {
            previous_tokens.remove(0);  // Keep only recent tokens
        }
    }

    assert_eq!(generated_tokens.len(), max_tokens,
               "Should generate exactly max_tokens");

    // 6. Test different sampling strategies produce different results
    let greedy = TokenSampler::new(SamplingConfig::greedy());
    let creative = TokenSampler::new(SamplingConfig::creative());

    let greedy_token = greedy.sample(&logits, &[])?;
    let creative_token1 = creative.sample(&logits, &[])?;
    let creative_token2 = creative.sample(&logits, &[])?;

    // Greedy should be deterministic (always highest logit)
    let greedy_token2 = greedy.sample(&logits, &[])?;
    assert_eq!(greedy_token, greedy_token2, "Greedy sampling should be deterministic");

    // Creative can vary (temperature > 0)
    // (Note: may occasionally be equal by chance)

    println!("âœ“ Full LLM pipeline simulation complete");
    println!("  - Tokenization: {} tokens", token_ids.len());
    println!("  - Generation: {} tokens", generated_tokens.len());
    println!("  - Greedy token: {}", greedy_token);
    println!("  - Creative tokens: {}, {}", creative_token1, creative_token2);

    Ok(())
}

#[test]
fn test_gguf_type_compatibility() {
    // Test that all GGUF types have consistent properties
    let types = vec![
        GgufType::F32,
        GgufType::F16,
        GgufType::Q4_0,
        GgufType::Q4_1,
        GgufType::Q5_0,
        GgufType::Q5_1,
        GgufType::Q8_0,
        GgufType::Q8_1,
    ];

    for gguf_type in types {
        let block_size = gguf_type.block_size();
        let type_size = gguf_type.type_size();

        assert!(block_size > 0, "Block size must be positive for {:?}", gguf_type);
        assert!(type_size > 0, "Type size must be positive for {:?}", gguf_type);

        // Block size should be reasonable (typically 32 or less)
        assert!(block_size <= 256, "Block size seems unreasonably large for {:?}", gguf_type);
    }
}

#[test]
fn test_tokenizer_special_tokens_integration() -> Result<()> {
    let mut tokenizer = BPETokenizer::new(32000);

    // Set special tokens
    tokenizer.set_bos_token(1);
    tokenizer.set_eos_token(2);
    tokenizer.set_pad_token(0);

    // Test encoding with BOS/EOS
    let text = "Test";
    let tokens_with_special = tokenizer.encode(text, true)?;
    let tokens_without_special = tokenizer.encode(text, false)?;

    // With special tokens should have at least BOS + tokens + EOS
    assert!(tokens_with_special.len() >= tokens_without_special.len(),
            "Encoding with special tokens should produce more tokens");

    // Test decoding skips special tokens
    let decoded = tokenizer.decode(&tokens_with_special)?;
    assert!(!decoded.is_empty(), "Decoding should produce text");

    Ok(())
}

#[test]
fn test_kv_cache_with_multiple_layers() {
    // Test KV-cache behaves correctly with transformer-like parameters
    let n_layers = 32;
    let d_model = 4096;
    let max_seq_len = 2048;
    let batch_size = 1;

    // Verify parameters are reasonable for a transformer
    assert!(n_layers >= 4 && n_layers <= 100, "Layer count should be reasonable");
    assert!(d_model % 128 == 0, "d_model should be multiple of 128 for efficiency");
    assert!(max_seq_len >= 512, "Context length should support reasonable sequences");

    // Memory calculation
    let bytes_per_layer = 2 * (max_seq_len * d_model * batch_size * 4); // keys + values, f32
    let total_memory_mb = (n_layers * bytes_per_layer) / (1024 * 1024);

    println!("KV-cache memory estimate: {} MB", total_memory_mb);
    assert!(total_memory_mb > 0, "Memory estimate should be positive");

    // For a 32-layer, 4096-dim, 2048-ctx model:
    // Should be around 2 * 32 * 2048 * 4096 * 4 bytes = ~2GB
    assert!(total_memory_mb < 10000, "Memory estimate should be reasonable (< 10GB)");
}

#[test]
fn test_sampling_strategy_consistency() -> Result<()> {
    // Test that all sampling strategies produce valid tokens
    let vocab_size = 32000;
    let mut logits = vec![0.0; vocab_size];

    // Create a peaked distribution
    logits[1000] = 5.0;
    logits[1001] = 4.0;
    logits[1002] = 3.0;
    for i in 1003..1100 {
        logits[i] = 2.0 - (i - 1003) as f32 * 0.01;
    }

    let strategies = vec![
        ("Greedy", SamplingConfig::greedy()),
        ("Standard", SamplingConfig::standard()),
        ("Creative", SamplingConfig::creative()),
        ("Precise", SamplingConfig::precise()),
        ("Min-p", SamplingConfig::min_p_recommended()),
    ];

    for (name, config) in strategies {
        let sampler = TokenSampler::new(config);

        // Sample 100 times
        for _ in 0..100 {
            let token = sampler.sample(&logits, &[])?;
            assert!(token >= 0 && token < vocab_size as i32,
                    "{} strategy produced invalid token: {}", name, token);
        }
    }

    Ok(())
}

#[test]
fn test_llm_component_interoperability() -> Result<()> {
    // Test that all LLM components can work together

    // 1. Tokenizer with realistic vocab size
    let vocab_size = 50257;  // GPT-2 vocab size
    let tokenizer = BPETokenizer::new(vocab_size);

    // 2. Test encoding/decoding roundtrip
    let test_texts = vec![
        "The quick brown fox jumps over the lazy dog",
        "Hello, world! 123",
        "Testing Unicode: ä½ å¥½ä¸–ç•Œ ðŸš€",
    ];

    for text in test_texts {
        let tokens = tokenizer.encode(text, false)?;
        assert!(!tokens.is_empty(), "Should produce tokens for: {}", text);

        // Verify all tokens are in vocab range
        for &token in &tokens {
            assert!(token >= 0 && token < vocab_size as i32,
                    "Token {} out of range for vocab size {}", token, vocab_size);
        }
    }

    // 3. Test sampling with various vocab sizes
    let sampler = TokenSampler::default();
    let logits = vec![0.1; vocab_size];
    let token = sampler.sample(&logits, &[])?;
    assert!(token >= 0 && token < vocab_size as i32);

    // 4. Test KV-cache parameters match model dimensions
    let n_layers = 12;  // GPT-2 small
    let d_model = 768;   // GPT-2 small

    // Verify compatibility
    assert!(d_model % 64 == 0, "d_model should be multiple of 64");
    assert!(n_layers > 0 && n_layers < 1000, "Layer count should be reasonable");

    Ok(())
}

#[test]
fn test_gguf_loader_error_handling() {
    // Test that GGUF type functions handle edge cases
    let q4_0 = GgufType::Q4_0;

    // Block size should be consistent
    assert_eq!(q4_0.block_size(), 32, "Q4_0 block size should be 32");

    // Type size calculation
    let size = q4_0.type_size();
    assert!(size > 0, "Type size must be positive");

    // Tensor size calculation with various shapes
    let shapes = vec![
        (1024, 4096),     // Small tensor
        (4096, 4096),     // Square tensor
        (32000, 4096),    // Embedding-like tensor
    ];

    for (rows, cols) in shapes {
        let total_elements = rows * cols;
        // Each Q4_0 block stores 32 elements in 18 bytes
        let expected_size = ((total_elements + 31) / 32) * 18;
        assert!(expected_size > 0, "Calculated size should be positive");
    }
}

#[test]
fn test_end_to_end_without_gpu() -> Result<()> {
    // Complete end-to-end test without requiring GPU
    println!("Starting end-to-end LLM pipeline test");

    // 1. Model configuration
    let vocab_size = 32000;
    let seq_len = 128;

    // 2. Initialize tokenizer
    let tokenizer = BPETokenizer::new(vocab_size);
    let input_text = "Once upon a time";
    let input_tokens = tokenizer.encode(input_text, false)?;
    println!("  Input: '{}' -> {} tokens", input_text, input_tokens.len());

    // 3. Simulate inference loop
    let mut context = input_tokens.clone();
    let max_new_tokens = 20;
    let sampler = TokenSampler::new(SamplingConfig::standard());

    for step in 0..max_new_tokens {
        // Simulate model logits (in real implementation, would run transformer)
        let mut logits = vec![0.0; vocab_size];
        for i in 0..vocab_size {
            logits[i] = (i as f32 * 0.001) + (rand::random::<f32>() - 0.5);
        }

        // Sample next token
        let next_token = sampler.sample(&logits, &context)?;
        context.push(next_token);

        // Prevent context from growing too large
        if context.len() > seq_len {
            context.remove(0);
        }

        if step % 5 == 0 {
            println!("  Step {}: context length = {}", step, context.len());
        }
    }

    println!("  Final context length: {}", context.len());
    println!("âœ“ End-to-end pipeline test complete");

    Ok(())
}

#[cfg(test)]
mod integration_edge_cases {
    use super::*;

    #[test]
    fn test_extreme_vocab_sizes() -> Result<()> {
        // Test with very small and very large vocab sizes
        let sizes = vec![100, 1000, 10000, 100000];

        for size in sizes {
            let tokenizer = BPETokenizer::new(size);
            let tokens = tokenizer.encode("test", false)?;

            // All tokens should be in vocab range
            for &token in &tokens {
                assert!(token >= 0 && token < size as i32,
                        "Token out of range for vocab size {}", size);
            }
        }

        Ok(())
    }

    #[test]
    fn test_long_sequence_handling() -> Result<()> {
        let tokenizer = BPETokenizer::new(50000);
        let sampler = TokenSampler::default();

        // Test with very long input
        let long_text = "word ".repeat(1000);
        let tokens = tokenizer.encode(&long_text, false)?;

        println!("Long text: {} chars -> {} tokens", long_text.len(), tokens.len());
        assert!(tokens.len() > 100, "Should produce many tokens");

        // Test sampling with long context
        let logits = vec![0.1; 50000];
        let next_token = sampler.sample(&logits, &tokens)?;
        assert!(next_token >= 0 && next_token < 50000);

        Ok(())
    }

    #[test]
    fn test_empty_and_minimal_inputs() -> Result<()> {
        let tokenizer = BPETokenizer::new(32000);

        // Empty string
        let empty_tokens = tokenizer.encode("", false)?;
        assert_eq!(empty_tokens.len(), 0, "Empty string should produce no tokens");

        // Single character
        let single = tokenizer.encode("a", false)?;
        assert!(single.len() > 0, "Single char should produce at least one token");

        // Whitespace only
        let whitespace = tokenizer.encode("   ", false)?;
        // Whitespace handling varies by tokenizer implementation

        Ok(())
    }
}
