{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Multi-Model LLM Consensus\n",
    "\n",
    "This notebook demonstrates advanced LLM orchestration with multi-model consensus strategies:\n",
    "\n",
    "1. Single model queries\n",
    "2. Multi-model voting\n",
    "3. Weighted consensus\n",
    "4. Chain-of-thought reasoning\n",
    "5. Self-consistency sampling\n",
    "6. Ensemble confidence analysis\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install requests numpy pandas matplotlib seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any\n",
    "from collections import Counter\n",
    "\n",
    "# Configuration\n",
    "API_KEY = \"your-api-key-here\"\n",
    "BASE_URL = \"http://localhost:8080\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Styling\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Single Model Query\n",
    "\n",
    "Basic query to a single LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(prompt: str, model: str = \"gpt-4\", temperature: float = 0.7, max_tokens: int = 200) -> Dict:\n",
    "    \"\"\"Query a single LLM model.\"\"\"\n",
    "    request = {\n",
    "        \"prompt\": prompt,\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{BASE_URL}/api/v1/llm/query\",\n",
    "        headers=headers,\n",
    "        json=request\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()['data']\n",
    "    else:\n",
    "        raise Exception(f\"Error {response.status_code}: {response.text}\")\n",
    "\n",
    "# Example query\n",
    "prompt = \"\"\"Analyze the following cybersecurity threat:\n",
    "\n",
    "An attacker has gained access to a web server through an unpatched vulnerability.\n",
    "They have uploaded a reverse shell and are attempting lateral movement.\n",
    "\n",
    "What are the top 3 immediate actions to take?\"\"\"\n",
    "\n",
    "result = query_llm(prompt, model=\"gpt-4\", temperature=0.3)\n",
    "\n",
    "print(\"Single Model Query Result:\\n\")\n",
    "print(f\"Model: {result['model_used']}\")\n",
    "print(f\"Response:\\n{result['text']}\\n\")\n",
    "print(f\"Tokens Used: {result['tokens_used']}\")\n",
    "print(f\"Cost: ${result['cost_usd']:.4f}\")\n",
    "print(f\"Latency: {result['latency_ms']:.0f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Multi-Model Voting\n",
    "\n",
    "Query multiple models and aggregate responses using majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-model consensus query\n",
    "consensus_request = {\n",
    "    \"prompt\": \"\"\"Question: What is the capital of Australia?\n",
    "    \n",
    "Provide only the city name, no explanation.\"\"\",\n",
    "    \"models\": [\n",
    "        {\"name\": \"gpt-4\", \"weight\": 1.0},\n",
    "        {\"name\": \"gpt-3.5-turbo\", \"weight\": 0.8},\n",
    "        {\"name\": \"claude-3\", \"weight\": 1.0},\n",
    "        {\"name\": \"palm-2\", \"weight\": 0.9}\n",
    "    ],\n",
    "    \"strategy\": \"majority_vote\",\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 50\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/api/v1/llm/consensus\",\n",
    "    headers=headers,\n",
    "    json=consensus_request\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()['data']\n",
    "    \n",
    "    print(\"Multi-Model Consensus Results:\\n\")\n",
    "    print(f\"Strategy: {result['strategy']}\")\n",
    "    print(f\"Consensus Answer: {result['consensus_text']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.2%}\\n\")\n",
    "    \n",
    "    print(\"Individual Model Responses:\")\n",
    "    print(f\"{'Model':<20} {'Response':<30} {'Latency (ms)':<15} {'Cost ($)'}\")\n",
    "    print(\"=\" * 85)\n",
    "    \n",
    "    for resp in result['individual_responses']:\n",
    "        print(f\"{resp['model']:<20} {resp['text']:<30} {resp['latency_ms']:>10.0f}     {resp['cost_usd']:>8.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*85}\")\n",
    "    print(f\"Total Cost: ${result['total_cost_usd']:.4f}\")\n",
    "    print(f\"Total Time: {result['total_time_ms']:.0f}ms\")\n",
    "    print(f\"Agreement Rate: {result['agreement_rate']:.1%}\")\n",
    "    \n",
    "    # Visualize consensus\n",
    "    if 'vote_distribution' in result:\n",
    "        votes = result['vote_distribution']\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(votes.keys(), votes.values(), color='steelblue', edgecolor='black')\n",
    "        plt.xlabel('Response')\n",
    "        plt.ylabel('Votes')\n",
    "        plt.title('Multi-Model Vote Distribution')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\nelse:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Weighted Consensus\n",
    "\n",
    "Use weighted voting based on model reliability for specific task types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technical/coding question with model specialization\n",
    "coding_prompt = \"\"\"Write a Python function to find the longest palindromic substring.\n",
    "Provide only the function definition, no explanation.\"\"\"\n",
    "\n",
    "weighted_request = {\n",
    "    \"prompt\": coding_prompt,\n",
    "    \"models\": [\n",
    "        {\"name\": \"gpt-4\", \"weight\": 1.5},          # Strong at coding\n",
    "        {\"name\": \"claude-3\", \"weight\": 1.3},       # Strong at reasoning\n",
    "        {\"name\": \"codex\", \"weight\": 2.0},          # Specialized for code\n",
    "        {\"name\": \"palm-2\", \"weight\": 1.0}          # General purpose\n",
    "    ],\n",
    "    \"strategy\": \"weighted_average\",\n",
    "    \"temperature\": 0.2,\n",
    "    \"max_tokens\": 300\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/api/v1/llm/consensus\",\n",
    "    headers=headers,\n",
    "    json=weighted_request\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()['data']\n",
    "    \n",
    "    print(\"Weighted Consensus Results:\\n\")\n",
    "    print(f\"Consensus Response:\\n{result['consensus_text']}\\n\")\n",
    "    print(f\"Weighted Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"Total Cost: ${result['total_cost_usd']:.4f}\")\n",
    "    print(f\"Total Time: {result['total_time_ms']:.0f}ms\\n\")\n",
    "    \n",
    "    # Visualize model weights and contributions\n",
    "    models = [r['model'] for r in result['individual_responses']]\n",
    "    weights = [r['weight'] for r in result['individual_responses']]\n",
    "    contributions = [r['contribution'] for r in result['individual_responses']]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Model weights\n",
    "    ax1.barh(models, weights, color='skyblue', edgecolor='black')\n",
    "    ax1.set_xlabel('Weight')\n",
    "    ax1.set_title('Model Weights')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Contributions to consensus\n",
    "    ax2.barh(models, contributions, color='lightcoral', edgecolor='black')\n",
    "    ax2.set_xlabel('Contribution to Consensus')\n",
    "    ax2.set_title('Model Contributions')\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\nelse:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Chain-of-Thought Reasoning\n",
    "\n",
    "Use chain-of-thought prompting for complex reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex reasoning problem\n",
    "cot_prompt = \"\"\"Problem: A satellite is in a circular orbit 500km above Earth's surface.\n",
    "Earth's radius is 6,371km and mass is 5.97 × 10^24 kg.\n",
    "Calculate the orbital velocity in km/s.\n",
    "\n",
    "Think step-by-step and show your reasoning.\"\"\"\n",
    "\n",
    "cot_request = {\n",
    "    \"prompt\": cot_prompt,\n",
    "    \"models\": [\n",
    "        {\"name\": \"gpt-4\", \"weight\": 1.0},\n",
    "        {\"name\": \"claude-3\", \"weight\": 1.0}\n",
    "    ],\n",
    "    \"strategy\": \"chain_of_thought\",\n",
    "    \"temperature\": 0.3,\n",
    "    \"max_tokens\": 500\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/api/v1/llm/consensus\",\n",
    "    headers=headers,\n",
    "    json=cot_request\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()['data']\n",
    "    \n",
    "    print(\"Chain-of-Thought Consensus:\\n\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(result['consensus_text'])\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"Reasoning Quality Score: {result['reasoning_quality']:.2f}/10\")\n",
    "    print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"\\nIndividual Model Reasoning Paths:\")\n",
    "    \n",
    "    for i, resp in enumerate(result['individual_responses'], 1):\n",
    "        print(f\"\\n--- Model {i}: {resp['model']} ---\")\n",
    "        print(resp['text'][:300] + \"...\" if len(resp['text']) > 300 else resp['text'])\nelse:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Self-Consistency Sampling\n",
    "\n",
    "Generate multiple responses and select the most consistent answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question with multiple valid reasoning paths\n",
    "sc_prompt = \"\"\"Question: If you have a 3-gallon jug and a 5-gallon jug,\n",
    "how can you measure exactly 4 gallons of water?\n",
    "\n",
    "Provide your answer as a sequence of steps.\"\"\"\n",
    "\n",
    "sc_request = {\n",
    "    \"prompt\": sc_prompt,\n",
    "    \"model\": \"gpt-4\",\n",
    "    \"strategy\": \"self_consistency\",\n",
    "    \"num_samples\": 5,\n",
    "    \"temperature\": 0.8,  # Higher temperature for diversity\n",
    "    \"max_tokens\": 300\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/api/v1/llm/consensus\",\n",
    "    headers=headers,\n",
    "    json=sc_request\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()['data']\n",
    "    \n",
    "    print(\"Self-Consistency Sampling Results:\\n\")\n",
    "    print(f\"Most Consistent Answer:\\n{result['consensus_text']}\\n\")\n",
    "    print(f\"Consistency Score: {result['consistency_score']:.2%}\")\n",
    "    print(f\"Number of Unique Approaches: {result['num_unique_answers']}\")\n",
    "    print(f\"\\nAll Generated Samples:\\n\")\n",
    "    \n",
    "    for i, sample in enumerate(result['samples'], 1):\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Sample {i} (Similarity: {sample['similarity_to_consensus']:.2%}):\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(sample['text'])\n",
    "        print()\n",
    "    \n",
    "    # Visualize consistency\n",
    "    similarities = [s['similarity_to_consensus'] for s in result['samples']]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(1, len(similarities)+1), similarities, color='mediumseagreen', edgecolor='black')\n",
    "    plt.axhline(y=np.mean(similarities), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(similarities):.2%}')\n",
    "    plt.xlabel('Sample Number')\n",
    "    plt.ylabel('Similarity to Consensus')\n",
    "    plt.title('Self-Consistency Sample Similarity')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\nelse:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Ensemble Confidence Analysis\n",
    "\n",
    "Analyze confidence and agreement across multiple models and strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative analysis across strategies\n",
    "def compare_strategies(prompt: str) -> pd.DataFrame:\n",
    "    \"\"\"Compare different consensus strategies on the same prompt.\"\"\"\n",
    "    strategies = ['majority_vote', 'weighted_average', 'best_of_n', 'unanimous']\n",
    "    results = []\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        request = {\n",
    "            \"prompt\": prompt,\n",
    "            \"models\": [\n",
    "                {\"name\": \"gpt-4\", \"weight\": 1.0},\n",
    "                {\"name\": \"gpt-3.5-turbo\", \"weight\": 0.8},\n",
    "                {\"name\": \"claude-3\", \"weight\": 1.0}\n",
    "            ],\n",
    "            \"strategy\": strategy,\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 100\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{BASE_URL}/api/v1/llm/consensus\",\n",
    "                headers=headers,\n",
    "                json=request\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()['data']\n",
    "                results.append({\n",
    "                    'Strategy': strategy,\n",
    "                    'Confidence': data['confidence'],\n",
    "                    'Agreement': data.get('agreement_rate', 0),\n",
    "                    'Cost': data['total_cost_usd'],\n",
    "                    'Time (ms)': data['total_time_ms'],\n",
    "                    'Answer': data['consensus_text'][:50] + '...'\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {strategy}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"\"\"What is the time complexity of binary search?\n",
    "Provide answer as Big-O notation only.\"\"\"\n",
    "\n",
    "comparison_df = compare_strategies(test_prompt)\n",
    "\n",
    "print(\"Strategy Comparison:\\n\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Confidence\n",
    "axes[0, 0].bar(comparison_df['Strategy'], comparison_df['Confidence'], \n",
    "               color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_ylabel('Confidence')\n",
    "axes[0, 0].set_title('Confidence by Strategy')\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Agreement\n",
    "axes[0, 1].bar(comparison_df['Strategy'], comparison_df['Agreement'], \n",
    "               color='lightcoral', edgecolor='black')\n",
    "axes[0, 1].set_ylabel('Agreement Rate')\n",
    "axes[0, 1].set_title('Agreement by Strategy')\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Cost\n",
    "axes[1, 0].bar(comparison_df['Strategy'], comparison_df['Cost'], \n",
    "               color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_ylabel('Cost (USD)')\n",
    "axes[1, 0].set_title('Cost by Strategy')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Latency\n",
    "axes[1, 1].bar(comparison_df['Strategy'], comparison_df['Time (ms)'], \n",
    "               color='plum', edgecolor='black')\n",
    "axes[1, 1].set_ylabel('Latency (ms)')\n",
    "axes[1, 1].set_title('Latency by Strategy')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\nStrategy Recommendations:\")\n",
    "print(\"=\" * 60)\n",
    "best_confidence = comparison_df.loc[comparison_df['Confidence'].idxmax()]\n",
    "print(f\"Highest Confidence: {best_confidence['Strategy']} ({best_confidence['Confidence']:.2%})\")\n",
    "\n",
    "best_agreement = comparison_df.loc[comparison_df['Agreement'].idxmax()]\n",
    "print(f\"Best Agreement: {best_agreement['Strategy']} ({best_agreement['Agreement']:.2%})\")\n",
    "\n",
    "lowest_cost = comparison_df.loc[comparison_df['Cost'].idxmin()]\n",
    "print(f\"Lowest Cost: {lowest_cost['Strategy']} (${lowest_cost['Cost']:.4f})\")\n",
    "\n",
    "fastest = comparison_df.loc[comparison_df['Time (ms)'].idxmin()]\n",
    "print(f\"Fastest: {fastest['Strategy']} ({fastest['Time (ms)']:.0f}ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 7. Real-World Use Case: Security Threat Analysis\n",
    "\n",
    "Apply multi-model consensus to analyze security threats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex security scenario\n",
    "security_prompt = \"\"\"Analyze this security incident:\n",
    "\n",
    "Logs show:\n",
    "- Multiple failed SSH login attempts from 203.0.113.42\n",
    "- Successful login after 127 attempts\n",
    "- Immediate privilege escalation attempt\n",
    "- Creation of new user account 'sysbackup'\n",
    "- Download of 2.3GB of data from /var/www/\n",
    "- Connection to external IP 198.51.100.15:4444\n",
    "\n",
    "Provide:\n",
    "1. Threat classification (severity 1-10)\n",
    "2. Attack vector analysis\n",
    "3. Top 3 immediate response actions\n",
    "4. Top 3 preventive measures for future\"\"\"\n",
    "\n",
    "security_request = {\n",
    "    \"prompt\": security_prompt,\n",
    "    \"models\": [\n",
    "        {\"name\": \"gpt-4\", \"weight\": 1.5},      # Strong reasoning\n",
    "        {\"name\": \"claude-3\", \"weight\": 1.5},   # Strong at analysis\n",
    "        {\"name\": \"palm-2\", \"weight\": 1.0}      # Diverse perspective\n",
    "    ],\n",
    "    \"strategy\": \"weighted_average\",\n",
    "    \"temperature\": 0.2,  # Low temp for factual analysis\n",
    "    \"max_tokens\": 600\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/api/v1/llm/consensus\",\n",
    "    headers=headers,\n",
    "    json=security_request\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()['data']\n",
    "    \n",
    "    print(\"Security Threat Analysis - Multi-Model Consensus\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    print(result['consensus_text'])\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nAnalysis Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"Model Agreement: {result['agreement_rate']:.1%}\")\n",
    "    print(f\"Total Analysis Time: {result['total_time_ms']:.0f}ms\")\n",
    "    print(f\"Total Cost: ${result['total_cost_usd']:.4f}\")\n",
    "    \n",
    "    # Show model-by-model comparison\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Individual Model Assessments:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for resp in result['individual_responses']:\n",
    "        print(f\"\\n{resp['model']} (Weight: {resp['weight']}, Cost: ${resp['cost_usd']:.4f}):\")\n",
    "        print(\"-\" * 80)\n",
    "        print(resp['text'][:400] + \"...\" if len(resp['text']) > 400 else resp['text'])\nelse:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial demonstrated advanced LLM orchestration:\n",
    "\n",
    "1. ✓ Single model queries with performance metrics\n",
    "2. ✓ Multi-model voting for improved accuracy\n",
    "3. ✓ Weighted consensus based on task-specific expertise\n",
    "4. ✓ Chain-of-thought reasoning for complex problems\n",
    "5. ✓ Self-consistency sampling for robust answers\n",
    "6. ✓ Ensemble confidence analysis across strategies\n",
    "7. ✓ Real-world security threat analysis\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "**When to use each strategy:**\n",
    "- **Majority Vote**: Simple questions with clear correct answers\n",
    "- **Weighted Average**: When models have known strengths for task types\n",
    "- **Chain-of-Thought**: Complex reasoning or mathematical problems\n",
    "- **Self-Consistency**: Multiple valid solution paths exist\n",
    "- **Best-of-N**: Quality over consensus matters most\n",
    "- **Unanimous**: Critical decisions requiring high certainty\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore streaming responses with `/api/v1/llm/stream`\n",
    "- See `04_pixel_processing.ipynb` for computer vision tasks\n",
    "- Check [LLM Documentation](../docs/API.md#llm-endpoints) for all options\n",
    "- Review cost optimization strategies in the [Architecture Guide](../docs/ARCHITECTURE.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
