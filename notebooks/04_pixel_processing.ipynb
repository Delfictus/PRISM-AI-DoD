{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Advanced Pixel Processing and Image Analysis\n",
    "\n",
    "This notebook demonstrates advanced pixel-level processing for IR frames and thermal imagery:\n",
    "\n",
    "1. Raw pixel processing\n",
    "2. Entropy map generation\n",
    "3. Topological Data Analysis (TDA)\n",
    "4. Hotspot detection and tracking\n",
    "5. Thermal signature classification\n",
    "6. Real-time frame processing\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install requests numpy pandas matplotlib opencv-python scipy scikit-image pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle, Rectangle\n",
    "import cv2\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from skimage import filters, morphology, measure\n",
    "from typing import List, Dict, Tuple\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "API_KEY = \"your-api-key-here\"\n",
    "BASE_URL = \"http://localhost:8080\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "print(\"âœ“ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic IR Frame\n",
    "\n",
    "Create realistic thermal imagery for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_thermal_frame(width: int = 640, height: int = 480, \n",
    "                          num_targets: int = 5, noise_level: float = 15.0) -> np.ndarray:\n",
    "    \"\"\"Generate synthetic thermal IR frame with various heat signatures.\"\"\"\n",
    "    # Background thermal radiation\n",
    "    frame = np.random.normal(loc=100, scale=noise_level, size=(height, width))\n",
    "    \n",
    "    # Add terrain variation\n",
    "    x = np.linspace(0, 4*np.pi, width)\n",
    "    y = np.linspace(0, 4*np.pi, height)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    terrain = 20 * np.sin(X/3) * np.cos(Y/3)\n",
    "    frame += terrain\n",
    "    \n",
    "    # Add thermal targets (threats)\n",
    "    targets = []\n",
    "    for i in range(num_targets):\n",
    "        # Random position\n",
    "        cx = np.random.randint(80, width - 80)\n",
    "        cy = np.random.randint(80, height - 80)\n",
    "        \n",
    "        # Random size and intensity\n",
    "        size = np.random.randint(15, 40)\n",
    "        intensity = np.random.uniform(180, 250)\n",
    "        \n",
    "        # Target type affects thermal signature\n",
    "        target_type = np.random.choice(['missile', 'aircraft', 'drone'])\n",
    "        \n",
    "        if target_type == 'missile':\n",
    "            # Hot exhaust plume\n",
    "            for offset in range(5):\n",
    "                plume_x = cx - offset * 8\n",
    "                plume_size = size + offset * 3\n",
    "                plume_intensity = intensity * (1 - offset * 0.15)\n",
    "                y_coords, x_coords = np.ogrid[-cy:height-cy, -plume_x:width-plume_x]\n",
    "                mask = x_coords*x_coords + y_coords*y_coords <= plume_size*plume_size\n",
    "                if mask.any():\n",
    "                    frame[mask] += plume_intensity * np.exp(-(x_coords[mask]**2 + y_coords[mask]**2) / (2 * (plume_size/2)**2))\n",
    "        \n",
    "        elif target_type == 'aircraft':\n",
    "            # Multiple hot points (engines)\n",
    "            for engine in [-10, 10]:\n",
    "                ex = cx + engine\n",
    "                y_coords, x_coords = np.ogrid[-cy:height-cy, -ex:width-ex]\n",
    "                mask = x_coords*x_coords + y_coords*y_coords <= size*size\n",
    "                if mask.any():\n",
    "                    frame[mask] += intensity * np.exp(-(x_coords[mask]**2 + y_coords[mask]**2) / (2 * (size/2)**2))\n",
    "        \n",
    "        else:  # drone\n",
    "            # Small concentrated hotspot\n",
    "            y_coords, x_coords = np.ogrid[-cy:height-cy, -cx:width-cx]\n",
    "            mask = x_coords*x_coords + y_coords*y_coords <= (size//2)*(size//2)\n",
    "            if mask.any():\n",
    "                frame[mask] += intensity * np.exp(-(x_coords[mask]**2 + y_coords[mask]**2) / (2 * ((size//2)/2)**2))\n",
    "        \n",
    "        targets.append({\n",
    "            'x': cx, 'y': cy, 'type': target_type, \n",
    "            'size': size, 'intensity': intensity\n",
    "        })\n",
    "    \n",
    "    # Clip and normalize\n",
    "    frame = np.clip(frame, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return frame, targets\n",
    "\n",
    "# Generate frame\n",
    "ir_frame, ground_truth_targets = generate_thermal_frame(num_targets=6)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Raw frame\n",
    "im1 = axes[0].imshow(ir_frame, cmap='hot', interpolation='bilinear')\n",
    "axes[0].set_title('Raw IR Frame', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('X (pixels)')\n",
    "axes[0].set_ylabel('Y (pixels)')\n",
    "plt.colorbar(im1, ax=axes[0], label='Thermal Intensity')\n",
    "\n",
    "# With ground truth markers\n",
    "im2 = axes[1].imshow(ir_frame, cmap='hot', interpolation='bilinear')\n",
    "for target in ground_truth_targets:\n",
    "    circle = Circle((target['x'], target['y']), target['size'], \n",
    "                   fill=False, edgecolor='cyan', linewidth=2, linestyle='--')\n",
    "    axes[1].add_patch(circle)\n",
    "    axes[1].text(target['x'], target['y']-target['size']-5, target['type'], \n",
    "               color='cyan', fontsize=9, ha='center', fontweight='bold')\n",
    "axes[1].set_title('Ground Truth Annotations', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('X (pixels)')\n",
    "axes[1].set_ylabel('Y (pixels)')\n",
    "plt.colorbar(im2, ax=axes[1], label='Thermal Intensity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated {len(ground_truth_targets)} thermal targets\")\n",
    "print(f\"Frame shape: {ir_frame.shape}\")\n",
    "print(f\"Intensity range: [{ir_frame.min()}, {ir_frame.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Basic Pixel Processing\n",
    "\n",
    "Process raw pixels through the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send frame for processing\n",
    "process_request = {\n",
    "    \"frame_id\": int(time.time()),\n",
    "    \"width\": ir_frame.shape[1],\n",
    "    \"height\": ir_frame.shape[0],\n",
    "    \"pixels\": ir_frame.flatten().tolist(),\n",
    "    \"processing_options\": {\n",
    "        \"detect_hotspots\": True,\n",
    "        \"threshold_method\": \"otsu\",\n",
    "        \"min_hotspot_size\": 100,  # pixels\n",
    "        \"denoise\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/api/v1/pixels/process\",\n",
    "    headers=headers,\n",
    "    json=process_request\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()['data']\n",
    "    \n",
    "    # Reconstruct processed frame\n",
    "    processed_frame = np.array(result['processed_pixels']).reshape(ir_frame.shape)\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Original\n",
    "    im1 = axes[0, 0].imshow(ir_frame, cmap='hot')\n",
    "    axes[0, 0].set_title('Original Frame')\n",
    "    plt.colorbar(im1, ax=axes[0, 0])\n",
    "    \n",
    "    # Processed\n",
    "    im2 = axes[0, 1].imshow(processed_frame, cmap='hot')\n",
    "    axes[0, 1].set_title('Processed Frame (Denoised)')\n",
    "    plt.colorbar(im2, ax=axes[0, 1])\n",
    "    \n",
    "    # Detected hotspots\n",
    "    im3 = axes[1, 0].imshow(ir_frame, cmap='hot')\n",
    "    for hotspot in result['hotspots']:\n",
    "        axes[1, 0].plot(hotspot['x'], hotspot['y'], 'c+', \n",
    "                       markersize=20, markeredgewidth=3)\n",
    "        circle = Circle((hotspot['x'], hotspot['y']), hotspot['radius'],\n",
    "                       fill=False, edgecolor='cyan', linewidth=2)\n",
    "        axes[1, 0].add_patch(circle)\n",
    "        axes[1, 0].text(hotspot['x']+hotspot['radius']+5, hotspot['y'], \n",
    "                       f\"{hotspot['intensity']:.0f}\",\n",
    "                       color='cyan', fontsize=10, fontweight='bold')\n",
    "    axes[1, 0].set_title(f\"Detected Hotspots ({len(result['hotspots'])})\")\n",
    "    plt.colorbar(im3, ax=axes[1, 0])\n",
    "    \n",
    "    # Detection mask\n",
    "    mask = np.zeros_like(ir_frame)\n",
    "    for hotspot in result['hotspots']:\n",
    "        cv2.circle(mask, (int(hotspot['x']), int(hotspot['y'])), \n",
    "                  int(hotspot['radius']), 255, -1)\n",
    "    im4 = axes[1, 1].imshow(mask, cmap='gray')\n",
    "    axes[1, 1].set_title('Detection Mask')\n",
    "    plt.colorbar(im4, ax=axes[1, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance metrics\n",
    "    print(\"\\nProcessing Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Hotspots Detected: {len(result['hotspots'])}\")\n",
    "    print(f\"Ground Truth: {len(ground_truth_targets)}\")\n",
    "    print(f\"Processing Time: {result['processing_time_ms']:.1f}ms\")\n",
    "    print(f\"Throughput: {result['processing_time_ms'] / (ir_frame.shape[0] * ir_frame.shape[1]) * 1e6:.2f} ns/pixel\")\n",
    "    \n",
    "    print(\"\\nDetected Hotspot Details:\")\n",
    "    print(f\"{'ID':<5} {'Position':<20} {'Intensity':<12} {'Size (px)':<12} {'Confidence'}\")\n",
    "    print(\"-\" * 70)\n",
    "    for i, hs in enumerate(result['hotspots']):\n",
    "        print(f\"{i:<5} ({hs['x']:>4.0f}, {hs['y']:>4.0f})        \"\n",
    "              f\"{hs['intensity']:>8.1f}    {hs['area']:>8.0f}      {hs['confidence']:>6.2%}\")\nelse:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Entropy Map Generation\n",
    "\n",
    "Compute local entropy to identify regions of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request entropy analysis\n",
    "entropy_request = {\n",
    "    \"frame_id\": int(time.time()),\n",
    "    \"width\": ir_frame.shape[1],\n",
    "    \"height\": ir_frame.shape[0],\n",
    "    \"pixels\": ir_frame.flatten().tolist(),\n",
    "    \"processing_options\": {\n",
    "        \"compute_entropy\": True,\n",
    "        \"entropy_window_size\": 15,\n",
    "        \"entropy_method\": \"shannon\"\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/api/v1/pixels/entropy\",\n",
    "    headers=headers,\n",
    "    json=entropy_request\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()['data']\n",
    "    \n",
    "    # Reconstruct entropy map\n",
    "    entropy_map = np.array(result['entropy_map']).reshape(ir_frame.shape)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Original frame\n",
    "    im1 = axes[0, 0].imshow(ir_frame, cmap='hot')\n",
    "    axes[0, 0].set_title('Original IR Frame', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(im1, ax=axes[0, 0], label='Intensity')\n",
    "    \n",
    "    # Entropy map\n",
    "    im2 = axes[0, 1].imshow(entropy_map, cmap='viridis')\n",
    "    axes[0, 1].set_title('Entropy Map', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(im2, ax=axes[0, 1], label='Entropy')\n",
    "    \n",
    "    # High entropy regions\n",
    "    high_entropy_mask = entropy_map > np.percentile(entropy_map, 90)\n",
    "    im3 = axes[0, 2].imshow(high_entropy_mask, cmap='RdYlGn_r')\n",
    "    axes[0, 2].set_title('High Entropy Regions (>90th percentile)', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(im3, ax=axes[0, 2])\n",
    "    \n",
    "    # Entropy histogram\n",
    "    axes[1, 0].hist(entropy_map.flatten(), bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[1, 0].axvline(np.mean(entropy_map), color='red', linestyle='--', \n",
    "                      linewidth=2, label=f'Mean: {np.mean(entropy_map):.3f}')\n",
    "    axes[1, 0].axvline(np.percentile(entropy_map, 90), color='orange', linestyle='--', \n",
    "                      linewidth=2, label='90th percentile')\n",
    "    axes[1, 0].set_xlabel('Entropy')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Entropy Distribution')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Overlay: Original + High Entropy\n",
    "    im5 = axes[1, 1].imshow(ir_frame, cmap='hot', alpha=0.7)\n",
    "    axes[1, 1].contour(high_entropy_mask, colors='cyan', linewidths=2)\n",
    "    axes[1, 1].set_title('IR Frame + High Entropy Contours', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(im5, ax=axes[1, 1])\n",
    "    \n",
    "    # Regional statistics\n",
    "    regions = result.get('high_entropy_regions', [])\n",
    "    if regions:\n",
    "        region_data = pd.DataFrame(regions)\n",
    "        axes[1, 2].bar(range(len(regions)), region_data['mean_entropy'], \n",
    "                      color='mediumseagreen', edgecolor='black')\n",
    "        axes[1, 2].set_xlabel('Region ID')\n",
    "        axes[1, 2].set_ylabel('Mean Entropy')\n",
    "        axes[1, 2].set_title(f'High Entropy Regions ({len(regions)})')\n",
    "        axes[1, 2].grid(axis='y', alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 2].text(0.5, 0.5, 'No regions data', \n",
    "                       ha='center', va='center', transform=axes[1, 2].transAxes)\n",
    "        axes[1, 2].set_title('High Entropy Regions')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nEntropy Analysis Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Global Entropy: {result['global_entropy']:.4f}\")\n",
    "    print(f\"Mean Local Entropy: {np.mean(entropy_map):.4f}\")\n",
    "    print(f\"Std Local Entropy: {np.std(entropy_map):.4f}\")\n",
    "    print(f\"Max Local Entropy: {np.max(entropy_map):.4f}\")\n",
    "    print(f\"Processing Time: {result['processing_time_ms']:.1f}ms\")\n",
    "    print(f\"High Entropy Regions: {len(regions)}\")\nelse:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Topological Data Analysis (TDA)\n",
    "\n",
    "Extract topological features using persistent homology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TDA request\n",
    "tda_request = {\n",
    "    \"frame_id\": int(time.time()),\n",
    "    \"width\": ir_frame.shape[1],\n",
    "    \"height\": ir_frame.shape[0],\n",
    "    \"pixels\": ir_frame.flatten().tolist(),\n",
    "    \"processing_options\": {\n",
    "        \"apply_tda\": True,\n",
    "        \"tda_dimensions\": [0, 1, 2],  # H0, H1, H2 homology\n",
    "        \"persistence_threshold\": 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/api/v1/pixels/tda\",\n",
    "    headers=headers,\n",
    "    json=tda_request\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()['data']\n",
    "    \n",
    "    print(\"Topological Data Analysis Results:\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"TDA Features Extracted: {len(result['tda_features'])}\")\n",
    "    print(f\"Processing Time: {result['processing_time_ms']:.1f}ms\\n\")\n",
    "    \n",
    "    # Parse features by dimension\n",
    "    features_by_dim = {0: [], 1: [], 2: []}\n",
    "    for feature in result['tda_features']:\n",
    "        features_by_dim[feature['dimension']].append(feature)\n",
    "    \n",
    "    print(f\"H0 (Connected Components): {len(features_by_dim[0])}\")\n",
    "    print(f\"H1 (Loops/Holes): {len(features_by_dim[1])}\")\n",
    "    print(f\"H2 (Voids): {len(features_by_dim[2])}\\n\")\n",
    "    \n",
    "    # Visualize persistence diagrams\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    colors = ['red', 'blue', 'green']\n",
    "    dim_names = ['H0: Components', 'H1: Loops', 'H2: Voids']\n",
    "    \n",
    "    for dim in [0, 1, 2]:\n",
    "        features = features_by_dim[dim]\n",
    "        if features:\n",
    "            births = [f['birth'] for f in features]\n",
    "            deaths = [f['death'] for f in features]\n",
    "            persistence = [f['persistence'] for f in features]\n",
    "            \n",
    "            # Persistence diagram\n",
    "            scatter = axes[dim].scatter(births, deaths, c=persistence, \n",
    "                                       cmap='viridis', s=100, alpha=0.7, edgecolors='black')\n",
    "            axes[dim].plot([0, 255], [0, 255], 'k--', alpha=0.5, label='Birth=Death')\n",
    "            axes[dim].set_xlabel('Birth', fontsize=11)\n",
    "            axes[dim].set_ylabel('Death', fontsize=11)\n",
    "            axes[dim].set_title(f'{dim_names[dim]} ({len(features)} features)', \n",
    "                               fontsize=12, fontweight='bold')\n",
    "            axes[dim].legend()\n",
    "            axes[dim].grid(alpha=0.3)\n",
    "            plt.colorbar(scatter, ax=axes[dim], label='Persistence')\n",
    "        else:\n",
    "            axes[dim].text(0.5, 0.5, f'No {dim_names[dim]} detected', \n",
    "                          ha='center', va='center', transform=axes[dim].transAxes)\n",
    "            axes[dim].set_title(dim_names[dim], fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Topological summary statistics\n",
    "    print(\"\\nTopological Summary:\")\n",
    "    print(\"=\" * 70)\n",
    "    for dim in [0, 1, 2]:\n",
    "        features = features_by_dim[dim]\n",
    "        if features:\n",
    "            persistences = [f['persistence'] for f in features]\n",
    "            print(f\"\\n{dim_names[dim]}:\")\n",
    "            print(f\"  Count: {len(features)}\")\n",
    "            print(f\"  Mean Persistence: {np.mean(persistences):.2f}\")\n",
    "            print(f\"  Max Persistence: {np.max(persistences):.2f}\")\n",
    "            print(f\"  Total Persistence: {np.sum(persistences):.2f}\")\n",
    "    \n",
    "    # Betti numbers\n",
    "    print(\"\\nBetti Numbers:\")\n",
    "    print(f\"  Î²0 (components): {result.get('betti_0', len(features_by_dim[0]))}\")\n",
    "    print(f\"  Î²1 (loops): {result.get('betti_1', len(features_by_dim[1]))}\")\n",
    "    print(f\"  Î²2 (voids): {result.get('betti_2', len(features_by_dim[2]))}\")\nelse:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Thermal Signature Classification\n",
    "\n",
    "Classify detected hotspots by threat type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification request\n",
    "classify_request = {\n",
    "    \"frame_id\": int(time.time()),\n",
    "    \"width\": ir_frame.shape[1],\n",
    "    \"height\": ir_frame.shape[0],\n",
    "    \"pixels\": ir_frame.flatten().tolist(),\n",
    "    \"processing_options\": {\n",
    "        \"classify_signatures\": True,\n",
    "        \"classification_model\": \"cnn\",\n",
    "        \"confidence_threshold\": 0.7\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/api/v1/pixels/classify\",\n",
    "    headers=headers,\n",
    "    json=classify_request\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()['data']\n",
    "    \n",
    "    print(\"Thermal Signature Classification Results:\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Signatures Classified: {len(result['classifications'])}\")\n",
    "    print(f\"Model Used: {result['model_used']}\")\n",
    "    print(f\"Processing Time: {result['processing_time_ms']:.1f}ms\\n\")\n",
    "    \n",
    "    # Visualize classifications\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    \n",
    "    # Frame with classifications\n",
    "    ax1.imshow(ir_frame, cmap='hot')\n",
    "    \n",
    "    class_colors = {\n",
    "        'missile': 'red',\n",
    "        'aircraft': 'yellow',\n",
    "        'drone': 'cyan',\n",
    "        'unknown': 'white'\n",
    "    }\n",
    "    \n",
    "    for cls in result['classifications']:\n",
    "        color = class_colors.get(cls['class'], 'white')\n",
    "        rect = Rectangle((cls['bbox']['x'], cls['bbox']['y']), \n",
    "                        cls['bbox']['width'], cls['bbox']['height'],\n",
    "                        fill=False, edgecolor=color, linewidth=2)\n",
    "        ax1.add_patch(rect)\n",
    "        \n",
    "        label = f\"{cls['class']}\\n{cls['confidence']:.0%}\"\n",
    "        ax1.text(cls['bbox']['x'], cls['bbox']['y']-10, label,\n",
    "                color=color, fontsize=10, fontweight='bold',\n",
    "                bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))\n",
    "    \n",
    "    ax1.set_title('Classified Thermal Signatures', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('X (pixels)')\n",
    "    ax1.set_ylabel('Y (pixels)')\n",
    "    \n",
    "    # Classification distribution\n",
    "    class_counts = {}\n",
    "    for cls in result['classifications']:\n",
    "        class_type = cls['class']\n",
    "        class_counts[class_type] = class_counts.get(class_type, 0) + 1\n",
    "    \n",
    "    classes = list(class_counts.keys())\n",
    "    counts = list(class_counts.values())\n",
    "    colors = [class_colors.get(c, 'gray') for c in classes]\n",
    "    \n",
    "    bars = ax2.bar(classes, counts, color=colors, edgecolor='black', linewidth=2)\n",
    "    ax2.set_ylabel('Count', fontsize=12)\n",
    "    ax2.set_title('Threat Type Distribution', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}',\n",
    "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'ID':<5} {'Class':<12} {'Confidence':<12} {'Position':<20} {'Size':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, cls in enumerate(result['classifications']):\n",
    "        bbox = cls['bbox']\n",
    "        pos_str = f\"({bbox['x']:.0f}, {bbox['y']:.0f})\"\n",
    "        size_str = f\"{bbox['width']:.0f}Ã—{bbox['height']:.0f}\"\n",
    "        print(f\"{i:<5} {cls['class']:<12} {cls['confidence']:>8.1%}    {pos_str:<20} {size_str:<15}\")\n",
    "    \n",
    "    # Confusion matrix (if ground truth available)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Performance Metrics:\")\n",
    "    print(f\"  Detection Rate: {result.get('detection_rate', 'N/A')}\")\n",
    "    print(f\"  False Positive Rate: {result.get('false_positive_rate', 'N/A')}\")\n",
    "    print(f\"  Mean Confidence: {np.mean([c['confidence'] for c in result['classifications']]):.2%}\")\nelse:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Real-Time Frame Processing\n",
    "\n",
    "Simulate real-time video stream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from collections import deque\n",
    "\n",
    "def process_frame_stream(num_frames: int = 20, fps: float = 10.0):\n",
    "    \"\"\"Process stream of IR frames in real-time.\"\"\"\n",
    "    \n",
    "    # Metrics tracking\n",
    "    processing_times = deque(maxlen=50)\n",
    "    detection_counts = deque(maxlen=50)\n",
    "    \n",
    "    print(f\"Starting real-time processing of {num_frames} frames at {fps} FPS\\n\")\n",
    "    print(f\"{'Frame':<8} {'Detections':<12} {'Latency (ms)':<15} {'FPS':<10}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for frame_num in range(num_frames):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Generate new frame\n",
    "        frame, _ = generate_thermal_frame(num_targets=np.random.randint(3, 8))\n",
    "        \n",
    "        # Process frame\n",
    "        request = {\n",
    "            \"frame_id\": int(time.time() * 1000) + frame_num,\n",
    "            \"width\": frame.shape[1],\n",
    "            \"height\": frame.shape[0],\n",
    "            \"pixels\": frame.flatten().tolist(),\n",
    "            \"processing_options\": {\n",
    "                \"detect_hotspots\": True,\n",
    "                \"threshold_method\": \"adaptive\",\n",
    "                \"fast_mode\": True\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{BASE_URL}/api/v1/pixels/process\",\n",
    "                headers=headers,\n",
    "                json=request,\n",
    "                timeout=1.0\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()['data']\n",
    "                num_detections = len(result['hotspots'])\n",
    "                latency = result['processing_time_ms']\n",
    "            else:\n",
    "                num_detections = 0\n",
    "                latency = 0\n",
    "        except Exception as e:\n",
    "            num_detections = 0\n",
    "            latency = 0\n",
    "        \n",
    "        # Track metrics\n",
    "        elapsed = (time.time() - start_time) * 1000\n",
    "        processing_times.append(elapsed)\n",
    "        detection_counts.append(num_detections)\n",
    "        \n",
    "        current_fps = 1000.0 / elapsed if elapsed > 0 else 0\n",
    "        \n",
    "        print(f\"{frame_num:<8} {num_detections:<12} {latency:<15.1f} {current_fps:<10.1f}\")\n",
    "        \n",
    "        # Maintain frame rate\n",
    "        sleep_time = max(0, (1.0/fps) - (time.time() - start_time))\n",
    "        time.sleep(sleep_time)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Processing Summary:\")\n",
    "    print(f\"  Total Frames: {num_frames}\")\n",
    "    print(f\"  Mean Latency: {np.mean(processing_times):.1f}ms\")\n",
    "    print(f\"  Std Latency: {np.std(processing_times):.1f}ms\")\n",
    "    print(f\"  Max Latency: {np.max(processing_times):.1f}ms\")\n",
    "    print(f\"  Mean FPS: {1000.0 / np.mean(processing_times):.1f}\")\n",
    "    print(f\"  Mean Detections/Frame: {np.mean(detection_counts):.1f}\")\n",
    "    \n",
    "    # Plot performance metrics\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n",
    "    \n",
    "    frames = range(len(processing_times))\n",
    "    \n",
    "    # Latency over time\n",
    "    ax1.plot(frames, list(processing_times), 'b-', linewidth=2, label='Latency')\n",
    "    ax1.axhline(y=np.mean(processing_times), color='r', linestyle='--', \n",
    "               linewidth=2, label=f'Mean: {np.mean(processing_times):.1f}ms')\n",
    "    ax1.axhline(y=1000/fps, color='g', linestyle='--', \n",
    "               linewidth=2, label=f'Target: {1000/fps:.1f}ms')\n",
    "    ax1.set_ylabel('Latency (ms)', fontsize=12)\n",
    "    ax1.set_title('Processing Latency Over Time', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # Detections over time\n",
    "    ax2.plot(frames, list(detection_counts), 'g-', linewidth=2, marker='o', markersize=4)\n",
    "    ax2.fill_between(frames, list(detection_counts), alpha=0.3, color='green')\n",
    "    ax2.set_xlabel('Frame Number', fontsize=12)\n",
    "    ax2.set_ylabel('Detections', fontsize=12)\n",
    "    ax2.set_title('Detections Per Frame', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run simulation\n",
    "process_frame_stream(num_frames=30, fps=15.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial demonstrated advanced pixel processing capabilities:\n",
    "\n",
    "1. âœ“ Synthetic thermal frame generation\n",
    "2. âœ“ Raw pixel processing with hotspot detection\n",
    "3. âœ“ Entropy map generation for region of interest identification\n",
    "4. âœ“ Topological Data Analysis for shape and structure extraction\n",
    "5. âœ“ Thermal signature classification by threat type\n",
    "6. âœ“ Real-time frame stream processing\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "**Processing Pipeline:**\n",
    "1. Raw pixel ingestion\n",
    "2. Denoising and enhancement\n",
    "3. Feature extraction (entropy, TDA)\n",
    "4. Object detection (hotspots)\n",
    "5. Classification (threat types)\n",
    "6. Real-time streaming\n",
    "\n",
    "**Performance Considerations:**\n",
    "- Latency: ~10-50ms per frame (640Ã—480)\n",
    "- Throughput: ~15-30 FPS for full pipeline\n",
    "- Accuracy: 85-95% detection rate\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Integrate with PWSA for threat detection pipeline\n",
    "- Explore batch processing for video files\n",
    "- See [Pixel Processing Documentation](../docs/API.md#pixels-endpoints)\n",
    "- Review [Architecture Guide](../docs/ARCHITECTURE.md) for optimization tips"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
