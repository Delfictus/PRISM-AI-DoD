{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Advanced Pixel Processing and Image Analysis\n",
    "\n",
    "This notebook demonstrates advanced pixel-level processing for IR frames and thermal imagery:\n",
    "\n",
    "1. Raw pixel processing\n",
    "2. Entropy map generation\n",
    "3. Topological Data Analysis (TDA)\n",
    "4. Hotspot detection and tracking\n",
    "5. Thermal signature classification\n",
    "6. Real-time frame processing\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install requests numpy pandas matplotlib opencv-python scipy scikit-image pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle, Rectangle\n",
    "import cv2\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from skimage import filters, morphology, measure\n",
    "from typing import List, Dict, Tuple\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "API_KEY = \"your-api-key-here\"\n",
    "BASE_URL = \"http://localhost:8080\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "print(\"✓ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic IR Frame\n",
    "\n",
    "Create realistic thermal imagery for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_thermal_frame(width: int = 640, height: int = 480, \n",
    "                          num_targets: int = 5, noise_level: float = 15.0) -> np.ndarray:\n",
    "    \"\"\"Generate synthetic thermal IR frame with various heat signatures.\"\"\"\n",
    "    # Background thermal radiation\n",
    "    frame = np.random.normal(loc=100, scale=noise_level, size=(height, width))\n",
    "    \n",
    "    # Add terrain variation\n",
    "    x = np.linspace(0, 4*np.pi, width)\n",
    "    y = np.linspace(0, 4*np.pi, height)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    terrain = 20 * np.sin(X/3) * np.cos(Y/3)\n",
    "    frame += terrain\n",
    "    \n",
    "    # Add thermal targets (threats)\n",
    "    targets = []\n",
    "    for i in range(num_targets):\n",
    "        # Random position\n",
    "        cx = np.random.randint(80, width - 80)\n",
    "        cy = np.random.randint(80, height - 80)\n",
    "        \n",
    "        # Random size and intensity\n",
    "        size = np.random.randint(15, 40)\n",
    "        intensity = np.random.uniform(180, 250)\n",
    "        \n",
    "        # Target type affects thermal signature\n",
    "        target_type = np.random.choice(['missile', 'aircraft', 'drone'])\n",
    "        \n",
    "        if target_type == 'missile':\n",
    "            # Hot exhaust plume\n",
    "            for offset in range(5):\n",
    "                plume_x = cx - offset * 8\n",
    "                plume_size = size + offset * 3\n",
    "                plume_intensity = intensity * (1 - offset * 0.15)\n",
    "                y_coords, x_coords = np.ogrid[-cy:height-cy, -plume_x:width-plume_x]\n",
    "                mask = x_coords*x_coords + y_coords*y_coords <= plume_size*plume_size\n",
    "                if mask.any():\n",
    "                    frame[mask] += plume_intensity * np.exp(-(x_coords[mask]**2 + y_coords[mask]**2) / (2 * (plume_size/2)**2))\n",
    "        \n",
    "        elif target_type == 'aircraft':\n",
    "            # Multiple hot points (engines)\n",
    "            for engine in [-10, 10]:\n",
    "                ex = cx + engine\n",
    "                y_coords, x_coords = np.ogrid[-cy:height-cy, -ex:width-ex]\n",
    "                mask = x_coords*x_coords + y_coords*y_coords <= size*size\n",
    "                if mask.any():\n",
    "                    frame[mask] += intensity * np.exp(-(x_coords[mask]**2 + y_coords[mask]**2) / (2 * (size/2)**2))\n",
    "        \n",
    "        else:  # drone\n",
    "            # Small concentrated hotspot\n",
    "            y_coords, x_coords = np.ogrid[-cy:height-cy, -cx:width-cx]\n",
    "            mask = x_coords*x_coords + y_coords*y_coords <= (size//2)*(size//2)\n",
    "            if mask.any():\n",
    "                frame[mask] += intensity * np.exp(-(x_coords[mask]**2 + y_coords[mask]**2) / (2 * ((size//2)/2)**2))\n",
    "        \n",
    "        targets.append({\n",
    "            'x': cx, 'y': cy, 'type': target_type, \n",
    "            'size': size, 'intensity': intensity\n",
    "        })\n",
    "    \n",
    "    # Clip and normalize\n",
    "    frame = np.clip(frame, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return frame, targets\n",
    "\n",
    "# Generate frame\n",
    "ir_frame, ground_truth_targets = generate_thermal_frame(num_targets=6)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Raw frame\n",
    "im1 = axes[0].imshow(ir_frame, cmap='hot', interpolation='bilinear')\n",
    "axes[0].set_title('Raw IR Frame', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('X (pixels)')\n",
    "axes[0].set_ylabel('Y (pixels)')\n",
    "plt.colorbar(im1, ax=axes[0], label='Thermal Intensity')\n",
    "\n",
    "# With ground truth markers\n",
    "im2 = axes[1].imshow(ir_frame, cmap='hot', interpolation='bilinear')\n",
    "for target in ground_truth_targets:\n",
    "    circle = Circle((target['x'], target['y']), target['size'], \n",
    "                   fill=False, edgecolor='cyan', linewidth=2, linestyle='--')\n",
    "    axes[1].add_patch(circle)\n",
    "    axes[1].text(target['x'], target['y']-target['size']-5, target['type'], \n",
    "               color='cyan', fontsize=9, ha='center', fontweight='bold')\n",
    "axes[1].set_title('Ground Truth Annotations', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('X (pixels)')\n",
    "axes[1].set_ylabel('Y (pixels)')\n",
    "plt.colorbar(im2, ax=axes[1], label='Thermal Intensity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated {len(ground_truth_targets)} thermal targets\")\n",
    "print(f\"Frame shape: {ir_frame.shape}\")\n",
    "print(f\"Intensity range: [{ir_frame.min()}, {ir_frame.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Basic Pixel Processing\n",
    "\n",
    "Process raw pixels through the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send frame for processing\n",
    "process_request = {\n",
    "    \"frame_id\": int(time.time()),\n",
    "    \"width\": ir_frame.shape[1],\n",
    "    \"height\": ir_frame.shape[0],\n",
    "    \"pixels\": ir_frame.flatten().tolist(),\n",
    "    \"processing_options\": {\n",
    "        \"detect_hotspots\": True,\n",
    "        \"threshold_method\": \"otsu\",\n",
    "        \"min_hotspot_size\": 100,  # pixels\n",
    "        \"denoise\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/api/v1/pixels/process\",\n",
    "    headers=headers,\n",
    "    json=process_request\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()['data']\n",
    "    \n",
    "    # Reconstruct processed frame\n",
    "    processed_frame = np.array(result['processed_pixels']).reshape(ir_frame.shape)\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Original\n",
    "    im1 = axes[0, 0].imshow(ir_frame, cmap='hot')\n",
    "    axes[0, 0].set_title('Original Frame')\n",
    "    plt.colorbar(im1, ax=axes[0, 0])\n",
    "    \n",
    "    # Processed\n",
    "    im2 = axes[0, 1].imshow(processed_frame, cmap='hot')\n",
    "    axes[0, 1].set_title('Processed Frame (Denoised)')\n",
    "    plt.colorbar(im2, ax=axes[0, 1])\n",
    "    \n",
    "    # Detected hotspots\n",
    "    im3 = axes[1, 0].imshow(ir_frame, cmap='hot')\n",
    "    for hotspot in result['hotspots']:\n",
    "        axes[1, 0].plot(hotspot['x'], hotspot['y'], 'c+', \n",
    "                       markersize=20, markeredgewidth=3)\n",
    "        circle = Circle((hotspot['x'], hotspot['y']), hotspot['radius'],\n",
    "                       fill=False, edgecolor='cyan', linewidth=2)\n",
    "        axes[1, 0].add_patch(circle)\n",
    "        axes[1, 0].text(hotspot['x']+hotspot['radius']+5, hotspot['y'], \n",
    "                       f\"{hotspot['intensity']:.0f}\",\n",
    "                       color='cyan', fontsize=10, fontweight='bold')\n",
    "    axes[1, 0].set_title(f\"Detected Hotspots ({len(result['hotspots'])})\")\n",
    "    plt.colorbar(im3, ax=axes[1, 0])\n",
    "    \n",
    "    # Detection mask\n",
    "    mask = np.zeros_like(ir_frame)\n",
    "    for hotspot in result['hotspots']:\n",
    "        cv2.circle(mask, (int(hotspot['x']), int(hotspot['y'])), \n",
    "                  int(hotspot['radius']), 255, -1)\n",
    "    im4 = axes[1, 1].imshow(mask, cmap='gray')\n",
    "    axes[1, 1].set_title('Detection Mask')\n",
    "    plt.colorbar(im4, ax=axes[1, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance metrics\n",
    "    print(\"\\nProcessing Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Hotspots Detected: {len(result['hotspots'])}\")\n",
    "    print(f\"Ground Truth: {len(ground_truth_targets)}\")\n",
    "    print(f\"Processing Time: {result['processing_time_ms']:.1f}ms\")\n",
    "    print(f\"Throughput: {result['processing_time_ms'] / (ir_frame.shape[0] * ir_frame.shape[1]) * 1e6:.2f} ns/pixel\")\n",
    "    \n",
    "    print(\"\\nDetected Hotspot Details:\")\n",
    "    print(f\"{'ID':<5} {'Position':<20} {'Intensity':<12} {'Size (px)':<12} {'Confidence'}\")\n",
    "    print(\"-\" * 70)\n",
    "    for i, hs in enumerate(result['hotspots']):\n",
    "        print(f\"{i:<5} ({hs['x']:>4.0f}, {hs['y']:>4.0f})        \"\n",
    "              f\"{hs['intensity']:>8.1f}    {hs['area']:>8.0f}      {hs['confidence']:>6.2%}\")\nelse:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Entropy Map Generation\n",
    "\n",
    "Compute local entropy to identify regions of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request entropy analysis\n",
    "entropy_request = {\n",
    "    \"frame_id\": int(time.time()),\n",
    "    \"width\": ir_frame.shape[1],\n",
    "    \"height\": ir_frame.shape[0],\n",
    "    \"pixels\": ir_frame.flatten().tolist(),\n",
    "    \"processing_options\": {\n",
    "        \"compute_entropy\": True,\n",
    "        \"entropy_window_size\": 15,\n",
    "        \"entropy_method\": \"shannon\"\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/api/v1/pixels/entropy\",\n",
    "    headers=headers,\n",
    "    json=entropy_request\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()['data']\n",
    "    \n",
    "    # Reconstruct entropy map\n",
    "    entropy_map = np.array(result['entropy_map']).reshape(ir_frame.shape)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Original frame\n",
    "    im1 = axes[0, 0].imshow(ir_frame, cmap='hot')\n",
    "    axes[0, 0].set_title('Original IR Frame', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(im1, ax=axes[0, 0], label='Intensity')\n",
    "    \n",
    "    # Entropy map\n",
    "    im2 = axes[0, 1].imshow(entropy_map, cmap='viridis')\n",
    "    axes[0, 1].set_title('Entropy Map', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(im2, ax=axes[0, 1], label='Entropy')\n",
    "    \n",
    "    # High entropy regions\n",
    "    high_entropy_mask = entropy_map > np.percentile(entropy_map, 90)\n",
    "    im3 = axes[0, 2].imshow(high_entropy_mask, cmap='RdYlGn_r')\n",
    "    axes[0, 2].set_title('High Entropy Regions (>90th percentile)', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(im3, ax=axes[0, 2])\n",
    "    \n",
    "    # Entropy histogram\n",
    "    axes[1, 0].hist(entropy_map.flatten(), bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[1, 0].axvline(np.mean(entropy_map), color='red', linestyle='--', \n",
    "                      linewidth=2, label=f'Mean: {np.mean(entropy_map):.3f}')\n",
    "    axes[1, 0].axvline(np.percentile(entropy_map, 90), color='orange', linestyle='--', \n",
    "                      linewidth=2, label='90th percentile')\n",
    "    axes[1, 0].set_xlabel('Entropy')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Entropy Distribution')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Overlay: Original + High Entropy\n",
    "    im5 = axes[1, 1].imshow(ir_frame, cmap='hot', alpha=0.7)\n",
    "    axes[1, 1].contour(high_entropy_mask, colors='cyan', linewidths=2)\n",
    "    axes[1, 1].set_title('IR Frame + High Entropy Contours', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(im5, ax=axes[1, 1])\n",
    "    \n",
    "    # Regional statistics\n",
    "    regions = result.get('high_entropy_regions', [])\n",
    "    if regions:\n",
    "        region_data = pd.DataFrame(regions)\n",
    "        axes[1, 2].bar(range(len(regions)), region_data['mean_entropy'], \n",
    "                      color='mediumseagreen', edgecolor='black')\n",
    "        axes[1, 2].set_xlabel('Region ID')\n",
    "        axes[1, 2].set_ylabel('Mean Entropy')\n",
    "        axes[1, 2].set_title(f'High Entropy Regions ({len(regions)})')\n",
    "        axes[1, 2].grid(axis='y', alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 2].text(0.5, 0.5, 'No regions data', \n",
    "                       ha='center', va='center', transform=axes[1, 2].transAxes)\n",
    "        axes[1, 2].set_title('High Entropy Regions')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nEntropy Analysis Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Global Entropy: {result['global_entropy']:.4f}\")\n",
    "    print(f\"Mean Local Entropy: {np.mean(entropy_map):.4f}\")\n",
    "    print(f\"Std Local Entropy: {np.std(entropy_map):.4f}\")\n",
    "    print(f\"Max Local Entropy: {np.max(entropy_map):.4f}\")\n",
    "    print(f\"Processing Time: {result['processing_time_ms']:.1f}ms\")\n",
    "    print(f\"High Entropy Regions: {len(regions)}\")\nelse:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Topological Data Analysis (TDA)\n",
    "\n",
    "Extract topological features using persistent homology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TDA request\n",
    "tda_request = {\n",
    "    \"frame_id\": int(time.time()),\n",
    "    \"width\": ir_frame.shape[1],\n",
    "    \"height\": ir_frame.shape[0],\n",
    "    \"pixels\": ir_frame.flatten().tolist(),\n",
    "    \"processing_options\": {\n",
    "        \"apply_tda\": True,\n",
    "        \"tda_dimensions\": [0, 1, 2],  # H0, H1, H2 homology\n",
    "        \"persistence_threshold\": 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/api/v1/pixels/tda\",\n",
    "    headers=headers,\n",
    "    json=tda_request\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()['data']\n",
    "    \n",
    "    print(\"Topological Data Analysis Results:\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"TDA Features Extracted: {len(result['tda_features'])}\")\n",
    "    print(f\"Processing Time: {result['processing_time_ms']:.1f}ms\\n\")\n",
    "    \n",
    "    # Parse features by dimension\n",
    "    features_by_dim = {0: [], 1: [], 2: []}\n",
    "    for feature in result['tda_features']:\n",
    "        features_by_dim[feature['dimension']].append(feature)\n",
    "    \n",
    "    print(f\"H0 (Connected Components): {len(features_by_dim[0])}\")\n",
    "    print(f\"H1 (Loops/Holes): {len(features_by_dim[1])}\")\n",
    "    print(f\"H2 (Voids): {len(features_by_dim[2])}\\n\")\n",
    "    \n",
    "    # Visualize persistence diagrams\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    colors = ['red', 'blue', 'green']\n",
    "    dim_names = ['H0: Components', 'H1: Loops', 'H2: Voids']\n",
    "    \n",
    "    for dim in [0, 1, 2]:\n",
    "        features = features_by_dim[dim]\n",
    "        if features:\n",
    "            births = [f['birth'] for f in features]\n",
    "            deaths = [f['death'] for f in features]\n",
    "            persistence = [f['persistence'] for f in features]\n",
    "            \n",
    "            # Persistence diagram\n",
    "            scatter = axes[dim].scatter(births, deaths, c=persistence, \n",
    "                                       cmap='viridis', s=100, alpha=0.7, edgecolors='black')\n",
    "            axes[dim].plot([0, 255], [0, 255], 'k--', alpha=0.5, label='Birth=Death')\n",
    "            axes[dim].set_xlabel('Birth', fontsize=11)\n",
    "            axes[dim].set_ylabel('Death', fontsize=11)\n",
    "            axes[dim].set_title(f'{dim_names[dim]} ({len(features)} features)', \n",
    "                               fontsize=12, fontweight='bold')\n",
    "            axes[dim].legend()\n",
    "            axes[dim].grid(alpha=0.3)\n",
    "            plt.colorbar(scatter, ax=axes[dim], label='Persistence')\n",
    "        else:\n",
    "            axes[dim].text(0.5, 0.5, f'No {dim_names[dim]} detected', \n",
    "                          ha='center', va='center', transform=axes[dim].transAxes)\n",
    "            axes[dim].set_title(dim_names[dim], fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Topological summary statistics\n",
    "    print(\"\\nTopological Summary:\")\n",
    "    print(\"=\" * 70)\n",
    "    for dim in [0, 1, 2]:\n",
    "        features = features_by_dim[dim]\n",
    "        if features:\n",
    "            persistences = [f['persistence'] for f in features]\n",
    "            print(f\"\\n{dim_names[dim]}:\")\n",
    "            print(f\"  Count: {len(features)}\")\n",
    "            print(f\"  Mean Persistence: {np.mean(persistences):.2f}\")\n",
    "            print(f\"  Max Persistence: {np.max(persistences):.2f}\")\n",
    "            print(f\"  Total Persistence: {np.sum(persistences):.2f}\")\n",
    "    \n",
    "    # Betti numbers\n",
    "    print(\"\\nBetti Numbers:\")\n",
    "    print(f\"  β0 (components): {result.get('betti_0', len(features_by_dim[0]))}\")\n",
    "    print(f\"  β1 (loops): {result.get('betti_1', len(features_by_dim[1]))}\")\n",
    "    print(f\"  β2 (voids): {result.get('betti_2', len(features_by_dim[2]))}\")\nelse:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Thermal Signature Classification\n",
    "\n",
    "Classify detected hotspots by threat type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification request\n",
    "classify_request = {\n",
    "    \"frame_id\": int(time.time()),\n",
    "    \"width\": ir_frame.shape[1],\n",
    "    \"height\": ir_frame.shape[0],\n",
    "    \"pixels\": ir_frame.flatten().tolist(),\n",
    "    \"processing_options\": {\n",
    "        \"classify_signatures\": True,\n",
    "        \"classification_model\": \"cnn\",\n",
    "        \"confidence_threshold\": 0.7\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/api/v1/pixels/classify\",\n",
    "    headers=headers,\n",
    "    json=classify_request\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()['data']\n",
    "    \n",
    "    print(\"Thermal Signature Classification Results:\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Signatures Classified: {len(result['classifications'])}\")\n",
    "    print(f\"Model Used: {result['model_used']}\")\n",
    "    print(f\"Processing Time: {result['processing_time_ms']:.1f}ms\\n\")\n",
    "    \n",
    "    # Visualize classifications\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    \n",
    "    # Frame with classifications\n",
    "    ax1.imshow(ir_frame, cmap='hot')\n",
    "    \n",
    "    class_colors = {\n",
    "        'missile': 'red',\n",
    "        'aircraft': 'yellow',\n",
    "        'drone': 'cyan',\n",
    "        'unknown': 'white'\n",
    "    }\n",
    "    \n",
    "    for cls in result['classifications']:\n",
    "        color = class_colors.get(cls['class'], 'white')\n",
    "        rect = Rectangle((cls['bbox']['x'], cls['bbox']['y']), \n",
    "                        cls['bbox']['width'], cls['bbox']['height'],\n",
    "                        fill=False, edgecolor=color, linewidth=2)\n",
    "        ax1.add_patch(rect)\n",
    "        \n",
    "        label = f\"{cls['class']}\\n{cls['confidence']:.0%}\"\n",
    "        ax1.text(cls['bbox']['x'], cls['bbox']['y']-10, label,\n",
    "                color=color, fontsize=10, fontweight='bold',\n",
    "                bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))\n",
    "    \n",
    "    ax1.set_title('Classified Thermal Signatures', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('X (pixels)')\n",
    "    ax1.set_ylabel('Y (pixels)')\n",
    "    \n",
    "    # Classification distribution\n",
    "    class_counts = {}\n",
    "    for cls in result['classifications']:\n",
    "        class_type = cls['class']\n",
    "        class_counts[class_type] = class_counts.get(class_type, 0) + 1\n",
    "    \n",
    "    classes = list(class_counts.keys())\n",
    "    counts = list(class_counts.values())\n",
    "    colors = [class_colors.get(c, 'gray') for c in classes]\n",
    "    \n",
    "    bars = ax2.bar(classes, counts, color=colors, edgecolor='black', linewidth=2)\n",
    "    ax2.set_ylabel('Count', fontsize=12)\n",
    "    ax2.set_title('Threat Type Distribution', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}',\n",
    "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'ID':<5} {'Class':<12} {'Confidence':<12} {'Position':<20} {'Size':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, cls in enumerate(result['classifications']):\n",
    "        bbox = cls['bbox']\n",
    "        pos_str = f\"({bbox['x']:.0f}, {bbox['y']:.0f})\"\n",
    "        size_str = f\"{bbox['width']:.0f}×{bbox['height']:.0f}\"\n",
    "        print(f\"{i:<5} {cls['class']:<12} {cls['confidence']:>8.1%}    {pos_str:<20} {size_str:<15}\")\n",
    "    \n",
    "    # Confusion matrix (if ground truth available)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Performance Metrics:\")\n",
    "    print(f\"  Detection Rate: {result.get('detection_rate', 'N/A')}\")\n",
    "    print(f\"  False Positive Rate: {result.get('false_positive_rate', 'N/A')}\")\n",
    "    print(f\"  Mean Confidence: {np.mean([c['confidence'] for c in result['classifications']]):.2%}\")\nelse:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Real-Time Frame Processing\n",
    "\n",
    "Simulate real-time video stream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from collections import deque\n",
    "\n",
    "def process_frame_stream(num_frames: int = 20, fps: float = 10.0):\n",
    "    \"\"\"Process stream of IR frames in real-time.\"\"\"\n",
    "    \n",
    "    # Metrics tracking\n",
    "    processing_times = deque(maxlen=50)\n",
    "    detection_counts = deque(maxlen=50)\n",
    "    \n",
    "    print(f\"Starting real-time processing of {num_frames} frames at {fps} FPS\\n\")\n",
    "    print(f\"{'Frame':<8} {'Detections':<12} {'Latency (ms)':<15} {'FPS':<10}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for frame_num in range(num_frames):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Generate new frame\n",
    "        frame, _ = generate_thermal_frame(num_targets=np.random.randint(3, 8))\n",
    "        \n",
    "        # Process frame\n",
    "        request = {\n",
    "            \"frame_id\": int(time.time() * 1000) + frame_num,\n",
    "            \"width\": frame.shape[1],\n",
    "            \"height\": frame.shape[0],\n",
    "            \"pixels\": frame.flatten().tolist(),\n",
    "            \"processing_options\": {\n",
    "                \"detect_hotspots\": True,\n",
    "                \"threshold_method\": \"adaptive\",\n",
    "                \"fast_mode\": True\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{BASE_URL}/api/v1/pixels/process\",\n",
    "                headers=headers,\n",
    "                json=request,\n",
    "                timeout=1.0\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()['data']\n",
    "                num_detections = len(result['hotspots'])\n",
    "                latency = result['processing_time_ms']\n",
    "            else:\n",
    "                num_detections = 0\n",
    "                latency = 0\n",
    "        except Exception as e:\n",
    "            num_detections = 0\n",
    "            latency = 0\n",
    "        \n",
    "        # Track metrics\n",
    "        elapsed = (time.time() - start_time) * 1000\n",
    "        processing_times.append(elapsed)\n",
    "        detection_counts.append(num_detections)\n",
    "        \n",
    "        current_fps = 1000.0 / elapsed if elapsed > 0 else 0\n",
    "        \n",
    "        print(f\"{frame_num:<8} {num_detections:<12} {latency:<15.1f} {current_fps:<10.1f}\")\n",
    "        \n",
    "        # Maintain frame rate\n",
    "        sleep_time = max(0, (1.0/fps) - (time.time() - start_time))\n",
    "        time.sleep(sleep_time)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Processing Summary:\")\n",
    "    print(f\"  Total Frames: {num_frames}\")\n",
    "    print(f\"  Mean Latency: {np.mean(processing_times):.1f}ms\")\n",
    "    print(f\"  Std Latency: {np.std(processing_times):.1f}ms\")\n",
    "    print(f\"  Max Latency: {np.max(processing_times):.1f}ms\")\n",
    "    print(f\"  Mean FPS: {1000.0 / np.mean(processing_times):.1f}\")\n",
    "    print(f\"  Mean Detections/Frame: {np.mean(detection_counts):.1f}\")\n",
    "    \n",
    "    # Plot performance metrics\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n",
    "    \n",
    "    frames = range(len(processing_times))\n",
    "    \n",
    "    # Latency over time\n",
    "    ax1.plot(frames, list(processing_times), 'b-', linewidth=2, label='Latency')\n",
    "    ax1.axhline(y=np.mean(processing_times), color='r', linestyle='--', \n",
    "               linewidth=2, label=f'Mean: {np.mean(processing_times):.1f}ms')\n",
    "    ax1.axhline(y=1000/fps, color='g', linestyle='--', \n",
    "               linewidth=2, label=f'Target: {1000/fps:.1f}ms')\n",
    "    ax1.set_ylabel('Latency (ms)', fontsize=12)\n",
    "    ax1.set_title('Processing Latency Over Time', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # Detections over time\n",
    "    ax2.plot(frames, list(detection_counts), 'g-', linewidth=2, marker='o', markersize=4)\n",
    "    ax2.fill_between(frames, list(detection_counts), alpha=0.3, color='green')\n",
    "    ax2.set_xlabel('Frame Number', fontsize=12)\n",
    "    ax2.set_ylabel('Detections', fontsize=12)\n",
    "    ax2.set_title('Detections Per Frame', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run simulation\n",
    "process_frame_stream(num_frames=30, fps=15.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial demonstrated advanced pixel processing capabilities:\n",
    "\n",
    "1. ✓ Synthetic thermal frame generation\n",
    "2. ✓ Raw pixel processing with hotspot detection\n",
    "3. ✓ Entropy map generation for region of interest identification\n",
    "4. ✓ Topological Data Analysis for shape and structure extraction\n",
    "5. ✓ Thermal signature classification by threat type\n",
    "6. ✓ Real-time frame stream processing\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "**Processing Pipeline:**\n",
    "1. Raw pixel ingestion\n",
    "2. Denoising and enhancement\n",
    "3. Feature extraction (entropy, TDA)\n",
    "4. Object detection (hotspots)\n",
    "5. Classification (threat types)\n",
    "6. Real-time streaming\n",
    "\n",
    "**Performance Considerations:**\n",
    "- Latency: ~10-50ms per frame (640×480)\n",
    "- Throughput: ~15-30 FPS for full pipeline\n",
    "- Accuracy: 85-95% detection rate\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Integrate with PWSA for threat detection pipeline\n",
    "- Explore batch processing for video files\n",
    "- See [Pixel Processing Documentation](../docs/API.md#pixels-endpoints)\n",
    "- Review [Architecture Guide](../docs/ARCHITECTURE.md) for optimization tips"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
