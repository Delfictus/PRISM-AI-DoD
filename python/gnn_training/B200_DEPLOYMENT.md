# 8x B200 Deployment Guide - Ultra-Fast Training

**Your Hardware:** 8x NVIDIA B200 (1,440 GB VRAM total)
**Expected Training Time:** 5-10 minutes ‚ö°
**Speedup:** ~8x faster than single GPU

---

## üöÄ **Quick Start (Multi-GPU Mode)**

### **Deploy to Your B200 Instance:**

```bash
# 1. Build Docker image (local machine)
cd /home/diddy/Desktop/PRISM-AI-DoD/python/gnn_training
bash build_and_push.sh

# 2. On your B200 RunPod instance
docker pull delfictus/prism-ai-world-record:latest

# 3. Upload training data
# (Use web interface or SCP)

# 4. Run MULTI-GPU training
bash run_multigpu.sh
```

**That's it!** All 8 B200 GPUs will be utilized automatically.

---

## ‚ö° **Performance Comparison**

| Mode | GPUs Used | Batch Size | Training Time | Command |
|------|-----------|------------|---------------|---------|
| **Single GPU** | 1 | 64 | 15-30 min | `bash run.sh` |
| **Multi-GPU** | 8 | 512 (64√ó8) | **5-10 min** ‚úÖ | `bash run_multigpu.sh` |

**Multi-GPU is RECOMMENDED for your hardware.**

---

## üìä **What Happens with Multi-GPU**

### **Automatic Scaling:**
1. **Data Parallelism** - Dataset split across 8 GPUs
2. **Gradient Synchronization** - All GPUs stay in sync
3. **Effective Batch Size** - 512 (64 per GPU √ó 8)
4. **Learning Rate Scaling** - Automatically adjusted

### **Expected Output:**
```
========================================================================
Multi-GPU GNN Training for Graph Coloring
========================================================================

Distributed Training Setup:
  World size: 8 GPUs
  Device: NVIDIA B200
  Total VRAM: 1536 GB

Datasets:
  Train: 12000 graphs (1500 per GPU)
  Val:   3000 graphs

Model:
  Parameters: 2,487,432
  Effective batch size: 512

Starting training for 100 epochs...

Epoch [1/100]
  [GPU 0] Batch [50/94] Loss: 2.1234 (color: 1.1, chromatic: 0.6, ...)
  Train Loss: 2.0543 (color: 1.0, chromatic: 0.5, ...)
  Val Loss:   1.8765 (color: 0.9, chromatic: 0.4, ...)
  ‚úÖ New best model saved (val_loss: 1.8765)
  Epoch time: 35.2s

...

‚úÖ Training complete!
  Total time: 6.3 minutes
  Best val loss: 1.2345
  GPUs used: 8
```

---

## üîß **Technical Details**

### **Multi-GPU Implementation:**
- **Framework:** PyTorch DistributedDataParallel (DDP)
- **Backend:** NCCL (NVIDIA Collective Communications)
- **Synchronization:** Gradient all-reduce after each batch
- **Sampler:** DistributedSampler (ensures no overlap)

### **Resource Utilization:**
- **VRAM per GPU:** ~30-40 GB (you have 192 GB!)
- **Total VRAM:** ~240-320 GB (you have 1,440 GB)
- **Headroom:** Plenty for even larger batches

### **Scaling Efficiency:**
- **Linear scaling expected:** 8 GPUs ‚âà 8√ó faster
- **Actual speedup:** ~7-8√ó (some communication overhead)
- **Training time:** ~5-10 minutes (vs 30-90 min on H100)

---

## üéØ **Optimizations for B200**

### **1. Already Optimized:**
‚úÖ Multi-GPU training (all 8 GPUs)
‚úÖ Mixed precision (AMP)
‚úÖ Gradient clipping
‚úÖ NCCL backend (fastest for NVIDIA)
‚úÖ DistributedSampler (no data duplication)

### **2. Optional: Increase Batch Size Further**

If you want even FASTER training:

**Edit `run_multigpu.sh` line 60:**
```bash
# Change from:
--batch-size 64 \

# To:
--batch-size 128 \  # 2x bigger, even faster
```

**New effective batch size:** 1024 (128 √ó 8)
**Training time:** ~3-7 minutes (even faster!)

---

## üìÅ **Files for Multi-GPU**

```
python/gnn_training/
‚îú‚îÄ‚îÄ train_multigpu.py      ‚úÖ NEW - Multi-GPU training script
‚îú‚îÄ‚îÄ run_multigpu.sh        ‚úÖ NEW - Automated multi-GPU execution
‚îú‚îÄ‚îÄ train.py               (Single GPU version)
‚îú‚îÄ‚îÄ run.sh                 (Single GPU version)
‚îî‚îÄ‚îÄ ... (other files)
```

---

## üöÄ **Deployment Steps**

### **Step 1: Build Docker Image** (local, one-time)
```bash
cd /home/diddy/Desktop/PRISM-AI-DoD/python/gnn_training
bash build_and_push.sh
```

### **Step 2: Deploy to B200 Instance**

**Use Custom Container on RunPod:**
- Image: `delfictus/prism-ai-world-record:latest`
- GPUs: 8x B200

### **Step 3: Upload Training Data**

```bash
# Package training data
cd /home/diddy/Desktop/PRISM-AI-DoD
tar -czf training_data.tar.gz training_data/

# Upload to B200 instance
scp training_data.tar.gz root@<pod-ip>:/workspace/

# On B200 instance
ssh root@<pod-ip>
cd /workspace
tar -xzf training_data.tar.gz
```

### **Step 4: Run Multi-GPU Training**

```bash
cd /workspace
bash run_multigpu.sh
```

**Automatic:**
- Detects all 8 GPUs
- Installs dependencies (if needed)
- Launches distributed training
- Exports to ONNX when done

### **Step 5: Download Trained Model**

```bash
# On B200 instance
tar -czf trained_model.tar.gz models/ checkpoints/

# Download to local
scp root@<pod-ip>:/workspace/trained_model.tar.gz .

# Extract
tar -xzf trained_model.tar.gz
cp models/coloring_gnn.onnx /home/diddy/Desktop/PRISM-AI-DoD/models/
```

### **Step 6: Stop Instance**

**CRITICAL:** Terminate immediately to avoid charges!

---

## üí∞ **Cost Estimate**

| Instance | Training Time | Est. $/hour | Total Cost |
|----------|---------------|-------------|------------|
| 8x B200 (multi-GPU) | 5-10 min | ~$15-30/hr | **$2-5** ‚úÖ |
| 8x B200 (single GPU) | 15-30 min | ~$15-30/hr | $8-15 |

**Multi-GPU is FASTER and CHEAPER!**

---

## üéØ **Comparison: Single vs Multi-GPU**

### **Single GPU Mode** (`bash run.sh`)
- Uses: 1 GPU
- Batch size: 64
- Training time: 15-30 min
- Cost: $8-15
- When to use: Testing, debugging

### **Multi-GPU Mode** (`bash run_multigpu.sh`) ‚úÖ
- Uses: All 8 GPUs
- Batch size: 512 (effective)
- Training time: 5-10 min
- Cost: $2-5
- When to use: **Production, final training**

---

## ‚ö†Ô∏è **Important Notes**

### **NCCL Backend Requirement:**
Multi-GPU training requires NCCL. Already included in Docker image.

### **Data Distribution:**
- Each GPU gets 1/8th of the data per epoch
- Total data seen: Same as single GPU
- Speedup comes from parallel processing

### **Model Synchronization:**
- Gradients synchronized after each batch
- All GPUs have identical model weights
- Final model saved from GPU 0

### **TensorBoard Monitoring:**
```bash
# On B200 instance
tensorboard --logdir ./logs --port 6006 --bind_all

# SSH tunnel from local machine
ssh -L 6006:localhost:6006 root@<pod-ip>

# Open browser: http://localhost:6006
```

---

## üêõ **Troubleshooting**

### **Issue: NCCL Error**
```bash
# Ensure NCCL is available
python -c "import torch; print(torch.distributed.is_nccl_available())"
# Should print: True
```

### **Issue: GPUs Not Detected**
```bash
# Check GPU count
nvidia-smi --query-gpu=name --format=csv,noheader | wc -l
# Should show: 8
```

### **Issue: Out of Memory**
Reduce batch size in `run_multigpu.sh`:
```bash
--batch-size 32  # Instead of 64
```

### **Issue: Training Stalls**
Check all GPUs are active:
```bash
watch -n 1 nvidia-smi
# All 8 GPUs should show activity
```

---

## ‚úÖ **Verification Checklist**

Before running:
- [ ] Docker image built and pushed
- [ ] B200 instance deployed (8 GPUs)
- [ ] Training data uploaded to `/workspace/training_data/`
- [ ] Verified GPU count: `nvidia-smi | grep B200 | wc -l` shows 8

After training:
- [ ] Training completed without errors
- [ ] ONNX model created: `models/coloring_gnn.onnx`
- [ ] Training time < 15 minutes
- [ ] Validation metrics look good
- [ ] Instance terminated

---

## üèÅ **Summary**

**Your B200 setup is PERFECT for this:**

‚úÖ **8 GPUs** ‚Üí Multi-GPU training (8√ó speedup)
‚úÖ **1,440 GB VRAM** ‚Üí Can handle massive batches
‚úÖ **B200 architecture** ‚Üí Latest & fastest

**Expected workflow:**
1. Build Docker image: `bash build_and_push.sh` (one-time)
2. Deploy B200 with custom image
3. Upload training data
4. Run: `bash run_multigpu.sh`
5. Wait 5-10 minutes
6. Download trained model
7. Stop instance

**Total time: ~20-30 minutes including setup**
**Training time: ~5-10 minutes**
**Cost: ~$2-5**

**This is THE BEST hardware for training this model. Use it!** üöÄ
