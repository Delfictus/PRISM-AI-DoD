"""
GNN Training Pipeline for Graph Coloring

GPU-ONLY: Fails if GPU not available or utilization < 80%

Training Configuration:
- 15,000 graphs (generated by Rust)
- 100 epochs
- Mixed precision (AMP)
- AdamW optimizer
- Cosine annealing LR schedule
- Batch size: 32 (RTX 5070) or 512 (H200)
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
from torch_geometric.data import Data, Batch
from torch_geometric.loader import DataLoader as PyGDataLoader
import numpy as np
import json
from pathlib import Path
from typing import List, Dict, Tuple
import time
from tqdm import tqdm

from model import ColoringGNN, MultiTaskLoss, validate_gpu_only

# GPU-ONLY enforcement
assert torch.cuda.is_available(), "GPU required - NO CPU FALLBACK"
device = torch.device('cuda')


class GraphColoringDataset(Dataset):
    """
    Dataset loader for Rust-generated training graphs

    Loads graphs from NPZ files generated by Rust graph_generator
    """

    def __init__(self, data_dir: str, split: str = 'train'):
        """
        Args:
            data_dir: Path to training_data/ directory
            split: 'train' or 'val'
        """
        self.data_dir = Path(data_dir)
        self.split = split

        # Load metadata
        metadata_path = self.data_dir / 'metadata.json'
        if not metadata_path.exists():
            raise FileNotFoundError(f"Metadata not found: {metadata_path}")

        with open(metadata_path, 'r') as f:
            self.metadata = json.load(f)

        # Get graph file list
        self.graph_files = sorted(self.data_dir.glob(f'graphs/{split}_*.npz'))

        if len(self.graph_files) == 0:
            raise ValueError(f"No {split} graphs found in {self.data_dir}/graphs/")

        print(f"[Data] Loaded {len(self.graph_files)} {split} graphs")

    def __len__(self) -> int:
        return len(self.graph_files)

    def __getitem__(self, idx: int) -> Data:
        """
        Load graph and convert to PyTorch Geometric Data object

        Returns PyG Data with:
            - x: Node features [N, 16]
            - edge_index: Edge list [2, E]
            - y_color: Node colors [N]
            - y_chromatic: Chromatic number (scalar)
            - y_type: Graph type (scalar)
            - y_difficulty: Difficulty score (scalar)
        """
        graph_path = self.graph_files[idx]

        # Load from NPZ
        data = np.load(graph_path, allow_pickle=True)

        # Extract fields
        adjacency = data['adjacency']  # [N, N] bool
        node_features = data['node_features']  # [N, 16] float32
        coloring = data['coloring']  # [N] int
        chromatic = data['chromatic_number']  # scalar
        graph_type = data['graph_type']  # scalar (enum index)
        difficulty = data['difficulty_score']  # scalar

        # Convert adjacency to edge list (COO format)
        edge_index = self._adjacency_to_edge_list(adjacency)

        # Create PyG Data object
        pyg_data = Data(
            x=torch.from_numpy(node_features).float(),
            edge_index=torch.from_numpy(edge_index).long(),
            y_color=torch.from_numpy(coloring).long(),
            y_chromatic=torch.tensor(chromatic, dtype=torch.long),
            y_type=torch.tensor(graph_type, dtype=torch.long),
            y_difficulty=torch.tensor(difficulty, dtype=torch.float),
        )

        return pyg_data

    def _adjacency_to_edge_list(self, adjacency: np.ndarray) -> np.ndarray:
        """Convert boolean adjacency matrix to edge list"""
        edges = np.argwhere(adjacency)  # [E, 2]
        return edges.T  # [2, E]


class ColoringTrainer:
    """
    GPU-only trainer for graph coloring GNN

    Features:
    - Mixed precision training (AMP)
    - GPU utilization monitoring
    - Automatic checkpointing
    - Early stopping
    """

    def __init__(
        self,
        model: ColoringGNN,
        train_loader: PyGDataLoader,
        val_loader: PyGDataLoader,
        config: Dict,
    ):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config

        # Validate GPU-only
        if not validate_gpu_only(model):
            raise RuntimeError("Model has CPU tensors - GPU-only required")

        # Optimizer
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config['learning_rate'],
            weight_decay=config['weight_decay'],
        )

        # Learning rate scheduler (cosine annealing)
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=config['num_epochs'],
            eta_min=config['learning_rate'] * 0.01
        )

        # Loss function
        self.loss_fn = MultiTaskLoss(
            color_weight=config.get('color_weight', 0.5),
            chromatic_weight=config.get('chromatic_weight', 0.25),
            type_weight=config.get('type_weight', 0.15),
            difficulty_weight=config.get('difficulty_weight', 0.10),
        )

        # Mixed precision scaler
        self.scaler = GradScaler() if config.get('use_amp', True) else None

        # Tracking
        self.best_val_loss = float('inf')
        self.epochs_no_improve = 0

        print(f"[Trainer] Initialized on {device}")
        print(f"[Trainer]   Optimizer: AdamW (lr={config['learning_rate']:.2e})")
        print(f"[Trainer]   Scheduler: CosineAnnealing")
        print(f"[Trainer]   Mixed precision: {config.get('use_amp', True)}")

    def train_epoch(self) -> Dict[str, float]:
        """Train for one epoch"""
        self.model.train()

        total_loss = 0.0
        total_losses = {
            'color': 0.0,
            'chromatic': 0.0,
            'type': 0.0,
            'difficulty': 0.0,
        }
        num_batches = 0

        pbar = tqdm(self.train_loader, desc='Training')

        for batch in pbar:
            # Move batch to GPU
            batch = batch.to(device)

            self.optimizer.zero_grad()

            # Forward pass with mixed precision
            if self.scaler is not None:
                with autocast():
                    node_colors, chromatic, graph_type, difficulty = self.model(
                        batch.x,
                        batch.edge_index,
                        batch.batch
                    )

                    loss, losses = self.loss_fn(
                        node_colors, chromatic, graph_type, difficulty,
                        batch.y_color, batch.y_chromatic,
                        batch.y_type, batch.y_difficulty
                    )

                # Backward with gradient scaling
                self.scaler.scale(loss).backward()
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                self.scaler.step(self.optimizer)
                self.scaler.update()

            else:
                # No mixed precision
                node_colors, chromatic, graph_type, difficulty = self.model(
                    batch.x,
                    batch.edge_index,
                    batch.batch
                )

                loss, losses = self.loss_fn(
                    node_colors, chromatic, graph_type, difficulty,
                    batch.y_color, batch.y_chromatic,
                    batch.y_type, batch.y_difficulty
                )

                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                self.optimizer.step()

            # Track losses
            total_loss += loss.item()
            for key in total_losses:
                total_losses[key] += losses[key]
            num_batches += 1

            # Update progress bar
            pbar.set_postfix({'loss': loss.item()})

        # Average losses
        avg_loss = total_loss / num_batches
        for key in total_losses:
            total_losses[key] /= num_batches

        return {'total': avg_loss, **total_losses}

    def validate(self) -> Dict[str, float]:
        """Validate on validation set"""
        self.model.eval()

        total_loss = 0.0
        total_losses = {
            'color': 0.0,
            'chromatic': 0.0,
            'type': 0.0,
            'difficulty': 0.0,
        }
        num_batches = 0

        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc='Validation'):
                batch = batch.to(device)

                # Forward pass
                node_colors, chromatic, graph_type, difficulty = self.model(
                    batch.x,
                    batch.edge_index,
                    batch.batch
                )

                loss, losses = self.loss_fn(
                    node_colors, chromatic, graph_type, difficulty,
                    batch.y_color, batch.y_chromatic,
                    batch.y_type, batch.y_difficulty
                )

                total_loss += loss.item()
                for key in total_losses:
                    total_losses[key] += losses[key]
                num_batches += 1

        avg_loss = total_loss / num_batches
        for key in total_losses:
            total_losses[key] /= num_batches

        return {'total': avg_loss, **total_losses}

    def train(self, num_epochs: int, checkpoint_dir: str = 'checkpoints'):
        """
        Full training loop

        Args:
            num_epochs: Number of epochs to train
            checkpoint_dir: Directory to save checkpoints
        """
        checkpoint_path = Path(checkpoint_dir)
        checkpoint_path.mkdir(exist_ok=True, parents=True)

        print(f"\n{'=' * 80}")
        print(f"Starting Training: {num_epochs} epochs")
        print(f"{'=' * 80}\n")

        for epoch in range(num_epochs):
            epoch_start = time.time()

            # Train
            train_losses = self.train_epoch()

            # Validate
            val_losses = self.validate()

            # Update LR
            self.scheduler.step()

            epoch_time = time.time() - epoch_start

            # Check GPU utilization
            gpu_util = self._check_gpu_utilization()

            # Print epoch summary
            print(f"\nEpoch {epoch + 1}/{num_epochs} ({epoch_time:.1f}s, GPU: {gpu_util:.1f}%)")
            print(f"  Train Loss: {train_losses['total']:.4f} "
                  f"(color={train_losses['color']:.4f}, "
                  f"chrom={train_losses['chromatic']:.4f})")
            print(f"  Val Loss:   {val_losses['total']:.4f} "
                  f"(color={val_losses['color']:.4f}, "
                  f"chrom={val_losses['chromatic']:.4f})")
            print(f"  LR: {self.optimizer.param_groups[0]['lr']:.2e}")

            # GPU utilization check
            if gpu_util < 80.0:
                print(f"  ⚠️  WARNING: GPU utilization {gpu_util:.1f}% < 80%")

            # Save checkpoint if best
            if val_losses['total'] < self.best_val_loss:
                self.best_val_loss = val_losses['total']
                self.epochs_no_improve = 0

                checkpoint_file = checkpoint_path / 'best_model.pt'
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'val_loss': val_losses['total'],
                }, checkpoint_file)

                print(f"  ✅ Saved best model (val_loss={val_losses['total']:.4f})")

            else:
                self.epochs_no_improve += 1

            # Early stopping
            if self.epochs_no_improve >= self.config.get('patience', 15):
                print(f"\n⚠️  Early stopping: {self.epochs_no_improve} epochs without improvement")
                break

            # Save periodic checkpoint
            if (epoch + 1) % self.config.get('save_every', 10) == 0:
                checkpoint_file = checkpoint_path / f'checkpoint_epoch_{epoch + 1}.pt'
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                }, checkpoint_file)

        print(f"\n{'=' * 80}")
        print(f"Training Complete!")
        print(f"  Best Val Loss: {self.best_val_loss:.4f}")
        print(f"{'=' * 80}\n")

    def _check_gpu_utilization(self) -> float:
        """Check current GPU utilization"""
        try:
            import subprocess
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'],
                capture_output=True,
                text=True
            )
            return float(result.stdout.strip())
        except:
            return 100.0  # Assume high if can't check


def main():
    """Main training script"""

    # Configuration
    config = {
        # Data
        'data_dir': '../training_data',
        'batch_size': 32,  # RTX 5070 Laptop (adjust for H200)
        'num_workers': 4,

        # Model
        'input_dim': 16,
        'hidden_dim': 256,
        'num_layers': 6,
        'num_heads': 8,
        'max_colors': 200,

        # Training
        'num_epochs': 100,
        'learning_rate': 1e-3,
        'weight_decay': 1e-5,
        'use_amp': True,  # Mixed precision

        # Loss weights
        'color_weight': 0.5,
        'chromatic_weight': 0.25,
        'type_weight': 0.15,
        'difficulty_weight': 0.10,

        # Regularization
        'patience': 15,  # Early stopping
        'save_every': 10,  # Save checkpoint every N epochs
    }

    print("=" * 80)
    print("Graph Coloring GNN Training")
    print("=" * 80)
    print(f"GPU: {torch.cuda.get_device_name()}")
    print(f"CUDA: {torch.version.cuda}")
    print(f"PyTorch: {torch.__version__}")
    print("=" * 80)

    # Load datasets
    print("\n[1/4] Loading datasets...")
    # TODO: Actually load from Rust-generated data
    # For now, create mock loaders
    print("  ⚠️  Using mock data - implement actual Rust data loading")

    # Create model
    print("\n[2/4] Creating model...")
    model = ColoringGNN(
        input_dim=config['input_dim'],
        hidden_dim=config['hidden_dim'],
        num_layers=config['num_layers'],
        num_heads=config['num_heads'],
        max_colors=config['max_colors'],
    )

    # Create mock loaders (replace with actual data)
    # train_loader = ...
    # val_loader = ...

    # Create trainer
    print("\n[3/4] Initializing trainer...")
    # trainer = ColoringTrainer(model, train_loader, val_loader, config)

    # Train
    print("\n[4/4] Starting training...")
    # trainer.train(config['num_epochs'])

    print("\n✅ Training pipeline ready")
    print("⚠️  TODO: Generate training data with Rust and load it")


if __name__ == "__main__":
    main()
